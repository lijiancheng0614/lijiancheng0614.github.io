<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lijiancheng0614</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://lijiancheng0614.github.io/"/>
  <updated>2025-08-08T03:42:05.822Z</updated>
  <id>http://lijiancheng0614.github.io/</id>
  
  <author>
    <name>Jiancheng Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>阿里社会招聘内推（长期有效）</title>
    <link href="http://lijiancheng0614.github.io/2021/07/10/2021_07_10_offer/"/>
    <id>http://lijiancheng0614.github.io/2021/07/10/2021_07_10_offer/</id>
    <published>2021-07-09T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.822Z</updated>
    
    <content type="html"><![CDATA[<p>非常靠谱的阿里社招内推~</p><ol type="1"><li>首先在 <a href="https://talent.alibaba.com/off-campus/" target="_blank" rel="noopener">阿里巴巴集团招聘官网</a> 选择意向的部门和岗位（不要自行投递哈）</li><li>然后将个人简历发送到 <a href="mailto:ljc250108@alibaba-inc.com" target="_blank" rel="noopener">ljc250108 AT alibaba-inc.com</a>，请备注需要内推到的部门及岗位，最好附上一段个人推荐理由（面试官看到可以加分哦），会第一时间进行内推，并可随时咨询面试进展</li></ol><a id="more"></a><p>保证第一时间处理简历，及时跟进面试流程，有问题可以随时咨询~</p><p>欢迎大家咨询和投递！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;非常靠谱的阿里社招内推~&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;首先在 &lt;a href=&quot;https://talent.alibaba.com/off-campus/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿里巴巴集团招聘官网&lt;/a&gt; 选择意向的部门和岗位（不要自行投递哈）&lt;/li&gt;
&lt;li&gt;然后将个人简历发送到 &lt;a href=&quot;mailto:ljc250108@alibaba-inc.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ljc250108 AT alibaba-inc.com&lt;/a&gt;，请备注需要内推到的部门及岗位，最好附上一段个人推荐理由（面试官看到可以加分哦），会第一时间进行内推，并可随时咨询面试进展&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Default" scheme="http://lijiancheng0614.github.io/categories/Default/"/>
    
    
      <category term="offer" scheme="http://lijiancheng0614.github.io/tags/offer/"/>
    
  </entry>
  
  <entry>
    <title>Python 覆盖率 Python coverage</title>
    <link href="http://lijiancheng0614.github.io/2019/08/15/2019_08_15_Python_coverage/"/>
    <id>http://lijiancheng0614.github.io/2019/08/15/2019_08_15_Python_coverage/</id>
    <published>2019-08-14T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.822Z</updated>
    
    <content type="html"><![CDATA[<p>覆盖率是度量测试完整性的一个手段，是测试有效性的一个度量。通过已执行代码表示，用于可靠性、稳定性以及性能的评测。</p><p><a href="http://coverage.readthedocs.io/en/latest/" target="_blank" rel="noopener">Coverage.py</a> 是一个用来统计 Python 代码覆盖率的工具。通过它可以检测测试代码对被测代码的覆盖率。</p><a id="more"></a><h2 id="安装">安装</h2><p>参考 <a href="https://coverage.readthedocs.io/en/latest/install.html" class="uri" target="_blank" rel="noopener">https://coverage.readthedocs.io/en/latest/install.html</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install coverage</span><br></pre></td></tr></table></figure><h2 id="准备-python-代码">准备 Python 代码</h2><p>如对于以下 Python 代码 <code>test_simple_class.py</code>，第 20 行和第 30 行代码在运行时并没有覆盖到。把注释了的代码去掉注释后覆盖率即可达到 100%。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""Test for SimpleClass.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> unittest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""A simple class.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num1, num2)</span>:</span></span><br><span class="line">        self.num1 = num1</span><br><span class="line">        self.num2 = num2</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sum</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Sum.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            int, the sum of num1 and num2.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.num1 + self.num2</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">difference</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Difference.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            int, the difference of num1 and num2.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.num1 &lt; self.num2:</span><br><span class="line">            <span class="keyword">return</span> self.num2 - self.num1</span><br><span class="line">        <span class="keyword">return</span> self.num1 - self.num2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSimpleClass</span><span class="params">(unittest.TestCase)</span>:</span></span><br><span class="line">    <span class="string">"""Test of SimpleClass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setUp</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.simple_class = SimpleClass(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># def test_sum(self):</span></span><br><span class="line">    <span class="comment">#     """Test sum.</span></span><br><span class="line">    <span class="comment">#     """</span></span><br><span class="line">    <span class="comment">#     self.assertEqual(self.simple_class.sum(), 3, 'test sum fail')</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_difference</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Test difference.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.assertEqual(self.simple_class.difference(), <span class="number">1</span>,</span><br><span class="line">                         <span class="string">'test difference fail'</span>)</span><br><span class="line">        <span class="comment"># self.simple_class.num1, self.simple_class.num2 = \</span></span><br><span class="line">        <span class="comment">#     self.simple_class.num2, self.simple_class.num1</span></span><br><span class="line">        <span class="comment"># self.assertEqual(self.simple_class.difference(), 1,</span></span><br><span class="line">        <span class="comment">#                  'test difference fail')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    unittest.main()</span><br></pre></td></tr></table></figure><h2 id="运行">运行</h2><h3 id="命令行方式">命令行方式</h3><p>统计行代码覆盖率：运行以下命令即可得到测试脚本 <code>test_simple_class.py</code> 的覆盖率文件 <code>.coverage</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coverage run test_simple_class.py</span><br></pre></td></tr></table></figure><p>更多常见参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coverage run test_simple_class.py arg1 arg2 --branch --include=PATH1,PATH2 --omit=PATH1,PATH2 --<span class="built_in">source</span>=SRC1,SRC2</span><br></pre></td></tr></table></figure><ul><li><code>arg1, arg2</code>：<code>test_simple_class.py</code> 执行时的参数</li><li><code>--branch</code>：统计分支覆盖率，默认只有行覆盖率</li><li><code>--include=PATH1,PATH2</code>：包含路径</li><li><code>--omit=PATH1,PATH2</code>：排除路径</li><li><code>--source=SRC1,SRC2</code>：覆盖率要统计的源信息</li></ul><h3 id="api-方式">API 方式</h3><p>可以在 Python 代码中直接调用 <code>coverage</code> 模块执行代码覆盖率的统计：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> coverage</span><br><span class="line"></span><br><span class="line">cov = coverage.coverage()</span><br><span class="line">cov.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># run your code</span></span><br><span class="line"></span><br><span class="line">cov.stop()</span><br><span class="line">cov.report()</span><br><span class="line">cov.html_report()</span><br></pre></td></tr></table></figure><h2 id="查看覆盖率结果">查看覆盖率结果</h2><ol type="1"><li><p>简单文本方式</p><p>根据覆盖率统计结果文件 <code>.coverage</code> 展示统计结果，<code>-m</code> 参数展示没被覆盖到的代码行号。</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coverage report -m</span><br></pre></td></tr></table></figure></p><p>输出结果如下：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Name                   Stmts   Miss  Cover   Missing</span><br><span class="line">----------------------------------------------------</span><br><span class="line">test_simple_class.py      18      2    89%   20, 30</span><br></pre></td></tr></table></figure></p></li><li><p>HTML 格式</p><p>根据覆盖率统计结果文件 <code>.coverage</code> 把结果以 HTML 格式保存在 <code>htmlcov</code> 文件夹里。</p><p>打开 <code>htmlcov/index.html</code> 即可看到覆盖率报告。</p></li><li><p>XML格式</p><p>根据覆盖率统计结果文件 <code>.coverage</code> 把结果以 XML 格式保存在 <code>coverage.xml</code> 文件里。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;覆盖率是度量测试完整性的一个手段，是测试有效性的一个度量。通过已执行代码表示，用于可靠性、稳定性以及性能的评测。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://coverage.readthedocs.io/en/latest/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Coverage.py&lt;/a&gt; 是一个用来统计 Python 代码覆盖率的工具。通过它可以检测测试代码对被测代码的覆盖率。&lt;/p&gt;
    
    </summary>
    
      <category term="Develop" scheme="http://lijiancheng0614.github.io/categories/Develop/"/>
    
    
      <category term="Python" scheme="http://lijiancheng0614.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python 搭建 Web 服务器</title>
    <link href="http://lijiancheng0614.github.io/2019/08/09/2019_08_09_Python_Web_Server/"/>
    <id>http://lijiancheng0614.github.io/2019/08/09/2019_08_09_Python_Web_Server/</id>
    <published>2019-08-08T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.822Z</updated>
    
    <content type="html"><![CDATA[<p>使用 Python 的 Web Server Gateway Interface (WSGI) 搭建 Web 服务器，参考官网：<a href="https://docs.python.org/2/library/wsgiref.html" class="uri" target="_blank" rel="noopener">https://docs.python.org/2/library/wsgiref.html</a></p><a id="more"></a><h2 id="简单的-web-服务器">简单的 Web 服务器</h2><p>保存以下代码为 <code>server.py</code>，运行 <code>python server.py</code>，访问 <code>http://127.0.0.1:8000</code> 就能看到效果了。</p><p>其中，</p><ul><li>environ：一个包含所有 HTTP 请求信息的 dict 对象</li><li>start_response：一个发送 HTTP 响应的函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wsgiref.simple_server <span class="keyword">import</span> make_server</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Every WSGI application must have an application object - a callable</span></span><br><span class="line"><span class="comment"># object that accepts two arguments. For that purpose, we're going to</span></span><br><span class="line"><span class="comment"># use a function (note that you're not limited to a function, you can</span></span><br><span class="line"><span class="comment"># use a class for example). The first argument passed to the function</span></span><br><span class="line"><span class="comment"># is a dictionary containing CGI-style environment variables and the</span></span><br><span class="line"><span class="comment"># second variable is the callable object (see PEP 333).</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world_app</span><span class="params">(environ, start_response)</span>:</span></span><br><span class="line">    status = <span class="string">'200 OK'</span>  <span class="comment"># HTTP Status</span></span><br><span class="line">    headers = [(<span class="string">'Content-type'</span>, <span class="string">'text/plain'</span>)]  <span class="comment"># HTTP Headers</span></span><br><span class="line">    start_response(status, headers)</span><br><span class="line">    <span class="comment"># The returned object is going to be printed</span></span><br><span class="line">    <span class="keyword">return</span> [<span class="string">"Hello World"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">httpd = make_server(<span class="string">''</span>, <span class="number">8000</span>, hello_world_app)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Serving on port 8000..."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Serve until process is killed</span></span><br><span class="line">httpd.serve_forever()</span><br></pre></td></tr></table></figure><h2 id="接受请求并返回结果的服务器">接受请求并返回结果的服务器</h2><h3 id="服务端">服务端</h3><p>运行 <code>python server.py</code> 即启动服务器。</p><ul><li>使用 <code>environ['REQUEST_METHOD']</code> 即可判断请求的类型。</li><li>使用 <code>environ['wsgi.input'].read(environ[CONTENT_LENGTH])</code> 即可读请求里的内容。</li></ul><p><code>server.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""Server.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> wsgiref.simple_server <span class="keyword">import</span> make_server</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_args</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Get arguments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Namespace, arguments.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=__doc__)</span><br><span class="line">    parser.add_argument(<span class="string">'--server_host'</span>,</span><br><span class="line">                        type=str,</span><br><span class="line">                        default=<span class="string">'0.0.0.0'</span>,</span><br><span class="line">                        help=<span class="string">'Server host.'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--server_port'</span>,</span><br><span class="line">                        type=int,</span><br><span class="line">                        default=<span class="number">8000</span>,</span><br><span class="line">                        help=<span class="string">'Server port.'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--images_dir'</span>,</span><br><span class="line">                        type=str,</span><br><span class="line">                        default=<span class="string">'upload_images/'</span>,</span><br><span class="line">                        help=<span class="string">'Images directory.'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ARGS = get_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">application</span><span class="params">(environ, start_response)</span>:</span></span><br><span class="line">    <span class="string">"""Application</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        environ: dict, environment values.</span></span><br><span class="line"><span class="string">        start_response: func, start_response function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        str, response.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    method = environ[<span class="string">'REQUEST_METHOD'</span>]</span><br><span class="line">    path = environ[<span class="string">'PATH_INFO'</span>]</span><br><span class="line">    status = <span class="string">'200 OK'</span></span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">'POST'</span> <span class="keyword">and</span> path == <span class="string">'/upload'</span>:</span><br><span class="line">        headers = [(<span class="string">'Content-type'</span>, <span class="string">'application/json'</span>)]</span><br><span class="line">        content_length = int(environ.get(<span class="string">'CONTENT_LENGTH'</span>, <span class="number">0</span>))</span><br><span class="line">        request_body = environ[<span class="string">'wsgi.input'</span>].read(content_length)</span><br><span class="line">        request_body = json.loads(request_body)</span><br><span class="line">        image_data = request_body[<span class="string">'image'</span>]</span><br><span class="line">        image_path = os.path.join(ARGS.images_dir,</span><br><span class="line">                                  <span class="string">'&#123;&#125;.jpg'</span>.format(time.time()))</span><br><span class="line">        image_data = base64.b64decode(image_data)</span><br><span class="line">        open(image_path, <span class="string">'wb'</span>).write(image_data)</span><br><span class="line">        response = &#123;<span class="string">'info'</span>: <span class="string">'Get text: &#123;&#125;'</span>.format(request_body[<span class="string">'text'</span>])&#125;</span><br><span class="line">        start_response(status, headers)</span><br><span class="line">        <span class="keyword">return</span> [json.dumps(response)]</span><br><span class="line">    headers = [(<span class="string">'Content-type'</span>, <span class="string">'text/plain'</span>)]</span><br><span class="line">    start_response(status, headers)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'Hello World'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""main.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(<span class="string">'-----------  Configuration Arguments -----------'</span>)</span><br><span class="line">    <span class="keyword">for</span> arg, value <span class="keyword">in</span> sorted(vars(ARGS).items()):</span><br><span class="line">        print(<span class="string">'&#123;&#125;: &#123;&#125;'</span>.format(arg, value))</span><br><span class="line">    print(<span class="string">'------------------------------------------------'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(ARGS.images_dir):</span><br><span class="line">        os.makedirs(ARGS.images_dir)</span><br><span class="line">    httpd = make_server(ARGS.server_host, ARGS.server_port, application)</span><br><span class="line">    print(<span class="string">'[INFO] Serving on port &#123;&#125;...'</span>.format(ARGS.server_port))</span><br><span class="line">    <span class="comment"># Serve until process is killed</span></span><br><span class="line">    httpd.serve_forever()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="客户端">客户端</h3><p>使用 <code>urllib2</code> 对服务进行 GET/POST 请求：运行 <code>python client.py</code> 即可进行简单的请求。</p><p><code>client.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""Request demo.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_args</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Get arguments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Namespace, arguments.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=__doc__)</span><br><span class="line">    parser.add_argument(<span class="string">'--server_url'</span>,</span><br><span class="line">                        type=str,</span><br><span class="line">                        default=<span class="string">'0.0.0.0:8000'</span>,</span><br><span class="line">                        help=<span class="string">'Server url.'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--image_path'</span>,</span><br><span class="line">                        type=str,</span><br><span class="line">                        default=<span class="string">'1.jpg'</span>,</span><br><span class="line">                        help=<span class="string">'Image path.'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_request</span><span class="params">(args)</span>:</span></span><br><span class="line">    <span class="string">"""Request.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        args: args.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># get</span></span><br><span class="line">    service_url = <span class="string">'http://&#123;&#125;'</span>.format(args.server_url)</span><br><span class="line">    print(<span class="string">'&#123;&#125; GET'</span>.format(time.ctime()))</span><br><span class="line">    request = urllib2.Request(service_url)</span><br><span class="line">    request.add_header(<span class="string">'Content-Type'</span>, <span class="string">'text/plain'</span>)</span><br><span class="line">    response = urllib2.urlopen(request, timeout=<span class="number">3000</span>).read()</span><br><span class="line">    print(response)</span><br><span class="line">    <span class="comment"># post</span></span><br><span class="line">    service_url = <span class="string">'http://&#123;&#125;/upload'</span>.format(args.server_url)</span><br><span class="line">    print(<span class="string">'&#123;&#125; POST &#123;&#125;'</span>.format(time.ctime(), args.image_path))</span><br><span class="line">    image_data = open(args.image_path, <span class="string">'rb'</span>).read()</span><br><span class="line">    image_data = base64.b64encode(image_data)</span><br><span class="line">    request_json = &#123;</span><br><span class="line">        <span class="string">'text'</span>: <span class="string">'Upload &#123;&#125;'</span>.format(args.image_path),</span><br><span class="line">        <span class="string">'image'</span>: image_data</span><br><span class="line">    &#125;</span><br><span class="line">    request_json = json.dumps(request_json)</span><br><span class="line">    request = urllib2.Request(service_url)</span><br><span class="line">    request.add_header(<span class="string">'Content-Type'</span>, <span class="string">'application/json'</span>)</span><br><span class="line">    response = urllib2.urlopen(request, request_json, <span class="number">3000</span>).read()</span><br><span class="line">    response = json.loads(response)</span><br><span class="line">    print(response)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""main.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    args = get_args()</span><br><span class="line">    print(<span class="string">'-----------  Configuration Arguments -----------'</span>)</span><br><span class="line">    <span class="keyword">for</span> arg, value <span class="keyword">in</span> sorted(vars(args).items()):</span><br><span class="line">        print(<span class="string">'&#123;&#125;: &#123;&#125;'</span>.format(arg, value))</span><br><span class="line">    print(<span class="string">'------------------------------------------------'</span>)</span><br><span class="line">    do_request(args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用 Python 的 Web Server Gateway Interface (WSGI) 搭建 Web 服务器，参考官网：&lt;a href=&quot;https://docs.python.org/2/library/wsgiref.html&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.python.org/2/library/wsgiref.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Develop" scheme="http://lijiancheng0614.github.io/categories/Develop/"/>
    
    
      <category term="Socket" scheme="http://lijiancheng0614.github.io/tags/Socket/"/>
    
      <category term="Python" scheme="http://lijiancheng0614.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>图像质量评估 Image Quality Assessment</title>
    <link href="http://lijiancheng0614.github.io/2019/07/30/2019_07_30_Image_Quality_Assessment/"/>
    <id>http://lijiancheng0614.github.io/2019/07/30/2019_07_30_Image_Quality_Assessment/</id>
    <published>2019-07-29T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.822Z</updated>
    
    <content type="html"><![CDATA[<p>图像质量评估（Image Quality Assessment, IQA）即评估图像的质量。</p><a id="more"></a><p>以下是最近一些方法及其代码。</p><h3 id="iqa">IQA</h3><ul><li><p>(ACM MM 2017, TMM 2019) No reference image quality assessment based Semantic Feature Aggregation<br><a href="https://github.com/lidq92/SFA" class="uri" target="_blank" rel="noopener">https://github.com/lidq92/SFA</a></p></li><li><p>(CVPR 2018) Blind Predicting Similar Quality Map for Image Quality Assessment</p></li><li><p>(TIP 2018) NIMA: Neural Image Assessment<br><a href="https://ai.googleblog.com/2017/12/introducing-nima-neural-image-assessment.html" class="uri" target="_blank" rel="noopener">https://ai.googleblog.com/2017/12/introducing-nima-neural-image-assessment.html</a><br><a href="https://github.com/idealo/image-quality-assessment" class="uri" target="_blank" rel="noopener">https://github.com/idealo/image-quality-assessment</a><br><a href="https://github.com/titu1994/neural-image-assessment" class="uri" target="_blank" rel="noopener">https://github.com/titu1994/neural-image-assessment</a><br><a href="https://github.com/kentsyx/Neural-IMage-Assessment" class="uri" target="_blank" rel="noopener">https://github.com/kentsyx/Neural-IMage-Assessment</a></p></li><li><p>(TIP 2018) Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment<br><a href="https://github.com/dmaniry/deepIQA" class="uri" target="_blank" rel="noopener">https://github.com/dmaniry/deepIQA</a><br><a href="https://github.com/lidq92/WaDIQaM" class="uri" target="_blank" rel="noopener">https://github.com/lidq92/WaDIQaM</a></p></li><li><p>(ICCV 2017) RankIQA: Learning from Rankings for No-reference Image Quality Assessment<br><a href="https://github.com/xialeiliu/RankIQA" class="uri" target="_blank" rel="noopener">https://github.com/xialeiliu/RankIQA</a></p></li></ul><h3 id="no-reference-image-quality-assessment">No-reference Image Quality Assessment</h3><ul><li><p>(CVPR 2018) Hallucinated-IQA: No-reference Image Quality Assessment via Adversarial Learning<br><a href="https://kwanyeelin.github.io/projects/HIQA/HIQA.html" class="uri" target="_blank" rel="noopener">https://kwanyeelin.github.io/projects/HIQA/HIQA.html</a></p></li><li><p>(BMVC 2018) Self-supervised Deep Multiple Choice Learning Network for Blind Image Quality Assessment</p></li><li><p>(CVPR 2014) Convolutional Neural Networks for No-Reference Image Quality Assessment<br><a href="https://github.com/lidq92/CNNIQA" class="uri" target="_blank" rel="noopener">https://github.com/lidq92/CNNIQA</a></p></li><li><p>(TIP 2012) No Reference Image Quality Assessment in the Spatial Domain<br><a href="https://github.com/krshrimali/No-Reference-Image-Quality-Assessment-using-BRISQUE-Model" class="uri" target="_blank" rel="noopener">https://github.com/krshrimali/No-Reference-Image-Quality-Assessment-using-BRISQUE-Model</a></p></li></ul><h3 id="aesthetics">Aesthetics</h3><ul><li><p>(CVPR 2019) Effective Aesthetics Prediction with Multi-level Spatially Pooled Features<br><a href="https://github.com/subpic/ava-mlsp" class="uri" target="_blank" rel="noopener">https://github.com/subpic/ava-mlsp</a></p></li><li><p>(PRIA 2019) Personalised aesthetics with residual adapters<br><a href="https://github.com/crp94/Personalised-aesthetic-assessment-using-residual-adapters" class="uri" target="_blank" rel="noopener">https://github.com/crp94/Personalised-aesthetic-assessment-using-residual-adapters</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像质量评估（Image Quality Assessment, IQA）即评估图像的质量。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>图像增强 Image Enhancement</title>
    <link href="http://lijiancheng0614.github.io/2019/07/21/2019_07_21_Image_Enhancement/"/>
    <id>http://lijiancheng0614.github.io/2019/07/21/2019_07_21_Image_Enhancement/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.822Z</updated>
    
    <content type="html"><![CDATA[<p>图像增强（Image Enhancement），其目的是要改善图像的视觉效果。</p><a id="more"></a><h2 id="自动色阶">自动色阶</h2><p>作用：自动调整图像中的黑白场。</p><p>原理：剪切每个通道中的阴影和高光部分，并将每个颜色通道中最亮或最暗的像素映射到纯白或纯黑；中间像素按比例重新分配分布。</p><p>运用：会增强图像中的对比度，因数像素值会增大。</p><p>特点：单独调整每个颜色通道，有可能会移去颜色或引入色痕。在像素平衡分布且需要以简单方式增加对比度的特定图像中，提供较好的效果。</p><h2 id="自动对比度">自动对比度</h2><p>作用：自动调整图像的对比度。</p><p>原理：剪切图像中的阴影和高光值，再将图像中的剩余部分的最亮和最暗像素映射到纯白或纯黑；中间像素按比例重新分配分布。</p><p>效果：会使高光看上去更亮，阴影看上去更暗。</p><p>特点：不会单独调整各个颜色通道，不会引入或消除色痕。</p><p>默认值情况：剪切白色/黑色像素的0.5%；也就是说忽略两个极端像素的0.5%</p><p>（可使用【色阶】或【曲线】对话框中的“自动颜色校正”选项来更改这个默认设置）</p><p>运用：可改进许多摄影或连续色调图像的外观，但无法改变单调颜色的图像属性。</p><h2 id="自动颜色">自动颜色</h2><p>原理：能过搜索图像来标识阴影、中间调、高光，从而校正图像的对比度和颜色。</p><p>默认值情况：使用“RGB128灰色（中度灰色）”这一目标颜色来中和中间调，并将阴影和高光剪切0.5%</p><p>（可使用【色阶】或【曲线】对话框中的“自动颜色校正”选项来更改这个默认设置）</p><h2 id="自动色彩均衡">自动色彩均衡</h2><ul><li>(IPOL 2012) Automatic Color Enhancement (ACE) and its Fast Implementation<br><a href="http://demo.ipol.im/demo/g_ace/" class="uri" target="_blank" rel="noopener">http://demo.ipol.im/demo/g_ace/</a></li></ul><p>自动色彩均衡：<a href="https://www.cnblogs.com/wangyong/p/9119394.html" class="uri" target="_blank" rel="noopener">https://www.cnblogs.com/wangyong/p/9119394.html</a></p><p>自动色彩均衡（ACE）快速算法：<a href="https://blog.csdn.net/zmshy2128/article/details/53470357" class="uri" target="_blank" rel="noopener">https://blog.csdn.net/zmshy2128/article/details/53470357</a></p><h2 id="自动白平衡">自动白平衡</h2><p><a href="https://blog.csdn.net/weixin_43379058/article/details/88606961" class="uri" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43379058/article/details/88606961</a></p><p>(ISCS 2005) A Novel Automatic White Balance Method For Digital Still Cameras</p><h2 id="直方图均衡化">直方图均衡化</h2><p>直方图均衡化是图像处理领域中利用图像直方图对对比度进行调整的方法。</p><p><a href="https://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html?highlight=equalizehist" class="uri" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html?highlight=equalizehist</a></p><h2 id="拉普拉斯算子">拉普拉斯算子</h2><p>使用中心为5的8邻域拉普拉斯算子与图像卷积可以达到锐化增强图像的目的</p><p><a href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#filter2d" class="uri" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#filter2d</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Mat kernel = (Mat_&lt;<span class="keyword">float</span>&gt;(<span class="number">3</span>, <span class="number">3</span>) &lt;&lt; <span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>);</span><br><span class="line">filter2D(image, imageEnhance, CV_8UC3, kernel);</span><br></pre></td></tr></table></figure><h2 id="对数log变换">对数Log变换</h2><p>对数变换可以将图像的低灰度值部分扩展，显示出低灰度部分更多的细节，将其高灰度值部分压缩，减少高灰度值部分的细节，从而达到强调图像低灰度部分的目的。</p><h2 id="伽马变换">伽马变换</h2><p>伽马变换主要用于图像的校正，将灰度过高或者灰度过低的图片进行修正，增强对比度。</p><h2 id="指数图像">指数图像</h2><p>指数变换的作用是扩展图像的高灰度级、压缩低灰度级，可用于亮度过高的图像。</p><h2 id="曝光过度问题处理">曝光过度问题处理</h2><p>对于曝光过度问题，可以通过计算当前图像的反相（255-image)，然后取当前图像和反相图像的较小者为当前像素位置的值。</p><h2 id="高反差保留">高反差保留</h2><p>将图像中颜色、明暗反差较大两部分的交界处保留下来。高反差保留 = 原图 - 高斯模糊图</p><p><a href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=gaussianblur#gaussianblur" class="uri" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=gaussianblur#gaussianblur</a></p><h2 id="偏色">偏色</h2><ul><li><p>(CVPR 2019) When Color Constancy Goes Wrong: Correcting Improperly White-Balanced Images<br><a href="http://130.63.97.192/WB_for_srgb_rendered_images/demo.html" class="uri" target="_blank" rel="noopener">http://130.63.97.192/WB_for_srgb_rendered_images/demo.html</a><br>效果不佳</p></li><li><p>(CVPR 2019) Quasi-Unsupervised Color Constancy<br><a href="https://github.com/claudio-unipv/quasi-unsupervised-cc" class="uri" target="_blank" rel="noopener">https://github.com/claudio-unipv/quasi-unsupervised-cc</a><br>效果一般</p></li><li><p>(CVPR 2018) Improving Color Reproduction Accuracy on Cameras<br><a href="https://karaimer.github.io/camera-color/" class="uri" target="_blank" rel="noopener">https://karaimer.github.io/camera-color/</a><br>Matlab 代码</p></li><li><p>(SIGGRAPH 2018) Exposure: A White-Box Photo Post-Processing Framework<br><a href="https://github.com/yuanming-hu/exposure" class="uri" target="_blank" rel="noopener">https://github.com/yuanming-hu/exposure</a><br>效果偏暗</p></li><li><p>(CVPR 2017) Fast Fourier Color Constancy<br><a href="https://github.com/google/ffcc" class="uri" target="_blank" rel="noopener">https://github.com/google/ffcc</a><br>Matlab Toolbox</p></li><li><p>(CVPR 2017) FC4: Fully Convolutional Color Constancy with Confidence-weighted Pooling<br><a href="https://github.com/yuanming-hu/fc4" class="uri" target="_blank" rel="noopener">https://github.com/yuanming-hu/fc4</a><br>论文 Exposure 一作的旧工作</p></li></ul><h2 id="低光图像增强">低光图像增强</h2><ul><li><p>EnlightenGAN: Deep Light Enhancement without Paired Supervision<br><a href="https://github.com/TAMU-VITA/EnlightenGAN" class="uri" target="_blank" rel="noopener">https://github.com/TAMU-VITA/EnlightenGAN</a></p></li><li><p>(CVPR 2019) Underexposed Photo Enhancement Using Deep Illumination Estimation<br><a href="https://github.com/wangruixing/DeepUPE" class="uri" target="_blank" rel="noopener">https://github.com/wangruixing/DeepUPE</a></p></li><li><p>(CVPR 2018) Learning to See in the Dark<br><a href="https://github.com/cchen156/Learning-to-See-in-the-Dark" class="uri" target="_blank" rel="noopener">https://github.com/cchen156/Learning-to-See-in-the-Dark</a></p></li><li><p>(BMVC 2018) MBLLEN: Low-light Image/Video Enhancement Using CNNs</p></li><li><p>(BMVC 2018) Deep Retinex Decomposition for Low-Light Enhancement<br><a href="https://github.com/weichen582/RetinexNet" class="uri" target="_blank" rel="noopener">https://github.com/weichen582/RetinexNet</a></p></li><li><p>(Pattern Recognition Letters 2018) LightenNet: A Convolutional Neural Network for weakly illuminated image enhancement<br><a href="https://github.com/Li-Chongyi/Low-Light-Codes" class="uri" target="_blank" rel="noopener">https://github.com/Li-Chongyi/Low-Light-Codes</a></p></li><li><ol start="2015" type="1"><li>LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement</li></ol></li></ul><h2 id="高动态图像增强">高动态图像增强</h2><p>从双边滤波到 HDRNet <a href="https://zhuanlan.zhihu.com/p/37404280" class="uri" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37404280</a></p><ul><li><p>(CVPR 2018 spotlight) Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs<br><a href="https://github.com/nothinglo/Deep-Photo-Enhancer" class="uri" target="_blank" rel="noopener">https://github.com/nothinglo/Deep-Photo-Enhancer</a></p></li><li><p>(SIGGRAPH 2017) Deep Bilateral Learning for Real-Time Image Enhancement<br><a href="https://groups.csail.mit.edu/graphics/hdrnet/" class="uri" target="_blank" rel="noopener">https://groups.csail.mit.edu/graphics/hdrnet/</a></p></li></ul><h2 id="颜色滤镜">颜色滤镜</h2><p>颜色滤镜即调色滤镜，也是最常见的滤镜，任何通过调节图像像素值的亮度、对比度、饱和度、色相等等方法，得到的不同于原图像颜色的效果，都统称为颜色滤镜。</p><p>LUT是Look Up Table的缩写，俗称为颜色查找表。颜色查找表有1D LUT、2D LUT、3D LUT三种。</p><p><a href="https://blog.csdn.net/trent1985/article/details/81101688" class="uri" target="_blank" rel="noopener">https://blog.csdn.net/trent1985/article/details/81101688</a></p><h2 id="未归类">未归类</h2><ul><li><p>(AAAI 2019) Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing<br><a href="https://github.com/rfuruta/pixelRL" class="uri" target="_blank" rel="noopener">https://github.com/rfuruta/pixelRL</a></p></li><li><p>(PRIA 2019) Personalised aesthetics with residual adapters<br><a href="https://github.com/crp94/Personalised-aesthetic-assessment-using-residual-adapters" class="uri" target="_blank" rel="noopener">https://github.com/crp94/Personalised-aesthetic-assessment-using-residual-adapters</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像增强（Image Enhancement），其目的是要改善图像的视觉效果。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>常用的量化策略及其实现</title>
    <link href="http://lijiancheng0614.github.io/2019/07/07/2019_07_07_Quantitative/"/>
    <id>http://lijiancheng0614.github.io/2019/07/07/2019_07_07_Quantitative/</id>
    <published>2019-07-06T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.822Z</updated>
    
    <content type="html"><![CDATA[<p>展示什么是量化投资，以及如何利用优矿量化平台编写各种策略。</p><a id="more"></a><h2 id="量化投资概述">量化投资概述</h2><h3 id="量化投资简介">量化投资简介</h3><p>量化投资：借助量化金融分析方法进行资产管理的一种投资方法。</p><p>量化金融分析方法：一种结合了金融数据、个人经验、数学模型及计算机技术的复杂金融建模及分析方法。</p><ul><li><p>金融数据：行情数据、高频数据、因子数据、新闻数据、地理信息数据、社交数据。</p></li><li><p>个人经验：基金经理的个人投资经验。</p></li><li><p>数学模型：统计分析、机器学习、深度学习。</p></li><li><p>计算机技术：简单计算、并行计算。</p></li></ul><h3 id="量化投资策略的类型">量化投资策略的类型</h3><p>量化投资策略有很多种分类方式，比如按照标的分类、按照技术分类、按照组合构建方式分类等。</p><p>按照标的分类：</p><ol type="1"><li><p>股票策略</p></li><li><p>基金策略</p><ul><li><p>指数基金</p></li><li><p>分级基金</p></li><li><p>股票型、债券型、货币型、衍生证券型基金</p></li></ul></li><li><p>期货策略</p><ul><li><p>股指期货</p></li><li><p>商品期货</p></li></ul></li><li><p>期权策略</p><ul><li><p>指数期权</p></li><li><p>个股期权</p></li></ul></li><li><p>债券策略</p><ul><li><p>国家债券</p></li><li><p>政府债券</p></li><li><p>上市公司债券</p></li><li><p>未上市公司债券</p></li><li><p>其他资产债券、组合债券</p></li></ul></li><li><p>海外资产策略</p><ul><li><p>直接投资海外市场证券</p></li><li><p>通过国内跟踪海外市场的基金来进行间接投资</p></li></ul></li></ol><h3 id="量化研究的流程">量化研究的流程</h3><p>获取数据 -&gt; 数据分析挖掘 -&gt; 构建信号 -&gt; 构建策略 -&gt; 回测 -&gt; 策略分析 -&gt; 模拟交易 -&gt; 实盘交易 -&gt; 获取数据</p><ol type="1"><li><p>获取数据</p><ul><li><p>公司财务数据</p></li><li><p>公司新闻数据</p></li><li><p>公司关联数据，以及产业上下游、主营业务、所属行业主题等数据</p></li><li><p>基本行情数据</p></li><li><p>高频数据、股票 Level-1 数据、股票 Level-2 数据、期货 Level-1 数据</p></li></ul></li><li><p>数据分析挖掘</p><ul><li><p>传统分析方法</p></li><li><p>新兴大数据、机器学习、数据挖掘方法</p></li></ul></li><li><p>构建信号</p><ul><li><p>在构建信号前进行数据处理、标准化、去极值、中性化</p></li><li><p>基础信号的研究、分组回测、ic、ir、衰减、行业分布</p></li><li><p>将基础信号合成复杂信号</p></li></ul></li><li><p>构建策略</p><ul><li><p>策略模板，兼容不同标的的策略，适用于股票、基金、期货等金融资产</p></li><li><p>兼容日线、分钟线甚至 Trick 级别的策略</p></li><li><p>方便好用的策略函数，获取历史行情、历史持仓信息、调仓记录等</p></li><li><p>支持各种订单类型：止盈止损单、限价单、市价单</p></li></ul></li><li><p>回测</p><ul><li><p>完美符合历史的真实行情</p></li><li><p>股票分红送转、除权除息处理</p></li><li><p>股票涨跌停处理</p></li><li><p>股票停复牌处理</p></li><li><p>市场冲击、交易滑点、手续费、期货保证金交易</p></li><li><p>大单笔成交处理等</p></li></ul></li><li><p>策略分析</p><ul><li><p>策略归因、风险归因、实时监控</p></li><li><p>订单分析、成交分析、持仓分析、交易行为分析</p></li><li><p>多策略分析，方便构建 FOF、MOM 类型的策略</p></li></ul></li><li><p>模拟交易</p><ul><li><p>接入实时行情、实时获取成交回报</p></li><li><p>篮子交易、算法交易</p></li><li><p>支持撤单处理</p></li><li><p>实时监控、实时归因分析</p></li></ul></li><li><p>实盘交易</p><ul><li><p>接入真实券商账户</p></li><li><p>极速行情、实时下单、实时获取订单回报</p></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;展示什么是量化投资，以及如何利用优矿量化平台编写各种策略。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Quantitative" scheme="http://lijiancheng0614.github.io/tags/Quantitative/"/>
    
  </entry>
  
  <entry>
    <title>TT100K</title>
    <link href="http://lijiancheng0614.github.io/2019/04/16/2019_04_16_TT100K/"/>
    <id>http://lijiancheng0614.github.io/2019/04/16/2019_04_16_TT100K/</id>
    <published>2019-04-15T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.815Z</updated>
    
    <content type="html"><![CDATA[<p>(CVPR 2016) Traffic-Sign Detection and Classification in the Wild</p><p>Paper: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf" class="uri" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf</a></p><p>Page: <a href="https://cg.cs.tsinghua.edu.cn/traffic-sign/" class="uri" target="_blank" rel="noopener">https://cg.cs.tsinghua.edu.cn/traffic-sign/</a></p><p>Code: <a href="http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/code.zip" class="uri" target="_blank" rel="noopener">http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/code.zip</a></p><a id="more"></a><p>我们做了两个贡献：</p><ol type="1"><li><p>我们从 10 万张腾讯街景全景图中创建了一个大型交通标志基准，超越了之前的基准。 它提供包含 30000 个交通标志实例的 100000 张图像。 这些图像涵盖了照度和天气条件的巨大变化。基准测试中的每个交通标志都标注了类别标签、边界框和像素掩码。 我们将此基准称为 Tsinghua-Tencent 100K。</p></li><li><p>我们演示了一个强大的端到端卷积神经网络（CNN）如何同时检测和分类交通标志。大多数先前的 CNN 图像处理解决方案针对占据图像的大部分的对象，并且这种网络不能很好地用于仅占据图像的一小部分的目标对象，比如这里的交通标志。实验结果表明了我们网络的稳健性及其对替代方案的优越性。</p></li></ol><p>We make two contributions:</p><ol type="1"><li><p>we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. It provides 100000 images containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K.</p></li><li><p>we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify traffic-signs. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives.</p></li></ol><h1 id="中文">中文</h1><h2 id="简介">简介</h2><h2 id="实验">实验</h2><h2 id="相关工作和讨论">相关工作和讨论</h2><h2 id="结论">结论</h2><h1 id="english">English</h1><h2 id="introduction">Introduction</h2><h2 id="related-work">Related work</h2><h3 id="traffic-sign-classification">Traffic Sign Classification</h3><h3 id="object-detection-by-cnns">Object Detection by CNNs</h3><h2 id="benchmark">Benchmark</h2><h3 id="data-collection">Data Collection</h3><h3 id="data-annotation">Data Annotation</h3><h3 id="dataset-statistics">Dataset Statistics</h3><h2 id="neural-network">Neural Network</h2><h3 id="architecture">Architecture</h3><h3 id="training">Training</h3><h2 id="results">Results</h2><h3 id="detection">Detection</h3><h3 id="simultaneous-detection-and-classification">Simultaneous detection and classification</h3><h2 id="conclusions">Conclusions</h2><h1 id="数据分析-data-analysis">数据分析 Data Analysis</h1><p>num_classes: 222 (0 background + 221 traffic signs)</p><table style="width:100%;"><colgroup><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"><col style="width: 10%"></colgroup><thead><tr class="header"><th><img src="signs/i1.png"></th><th><img src="signs/i2.png"></th><th><img src="signs/i3.png"></th><th><img src="signs/i4.png"></th><th><img src="signs/i5.png"></th><th><img src="signs/i6.png"></th><th><img src="signs/i7.png"></th><th><img src="signs/i8.png"></th><th><img src="signs/i9.png"></th><th><img src="signs/i10.png"></th></tr></thead><tbody><tr class="odd"><td>1 i1</td><td>2 i2</td><td>3 i3</td><td>4 i4</td><td>5 i5</td><td>6 i6</td><td>7 i7</td><td>8 i8</td><td>9 i9</td><td>10 i10</td></tr><tr class="even"><td><img src="signs/i11.png"></td><td><img src="signs/i12.png"></td><td><img src="signs/i13.png"></td><td><img src="signs/i14.png"></td><td><img src="signs/i15.png"></td><td><img src="signs/il50.png"></td><td><img src="signs/il60.png"></td><td><img src="signs/il70.png"></td><td><img src="signs/il80.png"></td><td><img src="signs/il90.png"></td></tr><tr class="odd"><td>11 i11</td><td>12 i12</td><td>13 i13</td><td>14 i14</td><td>15 i15</td><td>16 il50</td><td>17 il60</td><td>18 il70</td><td>19 il80</td><td>20 il90</td></tr><tr class="even"><td><img src="signs/il100.png"></td><td><img src="signs/il110.png"></td><td><img src="signs/ilx.png"></td><td><img src="signs/io.png"></td><td><img src="signs/ip.png"></td><td><img src="signs/p1.png"></td><td><img src="signs/p2.png"></td><td><img src="signs/p3.png"></td><td><img src="signs/p4.png"></td><td><img src="signs/p5.png"></td></tr><tr class="odd"><td>21 il100</td><td>22 il110</td><td>23 ilx</td><td>24 io</td><td>25 ip</td><td>26 p1</td><td>27 p2</td><td>28 p3</td><td>29 p4</td><td>30 p5</td></tr><tr class="even"><td><img src="signs/p6.png"></td><td><img src="signs/p7.png"></td><td><img src="signs/p8.png"></td><td><img src="signs/p9.png"></td><td><img src="signs/p10.png"></td><td><img src="signs/p11.png"></td><td><img src="signs/p12.png"></td><td><img src="signs/p13.png"></td><td><img src="signs/p14.png"></td><td><img src="signs/p15.png"></td></tr><tr class="odd"><td>31 p6</td><td>32 p7</td><td>33 p8</td><td>34 p9</td><td>35 p10</td><td>36 p11</td><td>37 p12</td><td>38 p13</td><td>39 p14</td><td>40 p15</td></tr><tr class="even"><td><img src="signs/p16.png"></td><td><img src="signs/p17.png"></td><td><img src="signs/p18.png"></td><td><img src="signs/p19.png"></td><td><img src="signs/p20.png"></td><td><img src="signs/p21.png"></td><td><img src="signs/p22.png"></td><td><img src="signs/p23.png"></td><td><img src="signs/p24.png"></td><td><img src="signs/p25.png"></td></tr><tr class="odd"><td>41 p16</td><td>42 p17</td><td>43 p18</td><td>44 p19</td><td>45 p20</td><td>46 p21</td><td>47 p22</td><td>48 p23</td><td>49 p24</td><td>50 p25</td></tr><tr class="even"><td><img src="signs/p26.png"></td><td><img src="signs/p27.png"></td><td><img src="signs/p28.png"></td><td><img src="signs/p29.png"></td><td><img src="signs/pa8.png"></td><td><img src="signs/pa10.png"></td><td><img src="signs/pa12.png"></td><td><img src="signs/pa13.png"></td><td><img src="signs/pa14.png"></td><td><img src="signs/pax.png"></td></tr><tr class="odd"><td>51 p26</td><td>52 p27</td><td>53 p28</td><td>54 p29</td><td>55 pa8</td><td>56 pa10</td><td>57 pa12</td><td>58 pa13</td><td>59 pa14</td><td>60 pax</td></tr><tr class="even"><td><img src="signs/pb.png"></td><td><img src="signs/pc.png"></td><td><img src="signs/pd.png"></td><td><img src="signs/pe.png"></td><td><img src="signs/pg.png"></td><td><img src="signs/ph1.5.png"></td><td><img src="signs/ph2.png"></td><td><img src="signs/ph2.1.png"></td><td><img src="signs/ph2.2.png"></td><td><img src="signs/ph2.4.png"></td></tr><tr class="odd"><td>61 pb</td><td>62 pc</td><td>63 pd</td><td>64 pe</td><td>65 pg</td><td>66 ph1.5</td><td>67 ph2</td><td>68 ph2.1</td><td>69 ph2.2</td><td>70 ph2.4</td></tr><tr class="even"><td><img src="signs/ph2.5.png"></td><td><img src="signs/ph2.6.png"></td><td><img src="signs/ph2.8.png"></td><td><img src="signs/ph2.9.png"></td><td><img src="signs/ph3.png"></td><td><img src="signs/ph3.2.png"></td><td><img src="signs/ph3.3.png"></td><td><img src="signs/ph3.5.png"></td><td><img src="signs/ph3.8.png"></td><td><img src="signs/ph4.png"></td></tr><tr class="odd"><td>71 ph2.5</td><td>72 ph2.6</td><td>73 ph2.8</td><td>74 ph2.9</td><td>75 ph3</td><td>76 ph3.2</td><td>77 ph3.3</td><td>78 ph3.5</td><td>79 ph3.8</td><td>80 ph4</td></tr><tr class="even"><td><img src="signs/ph4.2.png"></td><td><img src="signs/ph4.3.png"></td><td><img src="signs/ph4.4.png"></td><td><img src="signs/ph4.5.png"></td><td><img src="signs/ph4.8.png"></td><td><img src="signs/ph5.png"></td><td><img src="signs/ph5.3.png"></td><td><img src="signs/ph5.5.png"></td><td><img src="signs/phx.png"></td><td><img src="signs/pl0.png"></td></tr><tr class="odd"><td>81 ph4.2</td><td>82 ph4.3</td><td>83 ph4.4</td><td>84 ph4.5</td><td>85 ph4.8</td><td>86 ph5</td><td>87 ph5.3</td><td>88 ph5.5</td><td>89 phx</td><td>90 pl0</td></tr><tr class="even"><td><img src="signs/pl3.png"></td><td><img src="signs/pl4.png"></td><td><img src="signs/pl5.png"></td><td><img src="signs/pl10.png"></td><td><img src="signs/pl15.png"></td><td><img src="signs/pl20.png"></td><td><img src="signs/pl25.png"></td><td><img src="signs/pl30.png"></td><td><img src="signs/pl35.png"></td><td><img src="signs/pl40.png"></td></tr><tr class="odd"><td>91 pl3</td><td>92 pl4</td><td>93 pl5</td><td>94 pl10</td><td>95 pl15</td><td>96 pl20</td><td>97 pl25</td><td>98 pl30</td><td>99 pl35</td><td>100 pl40</td></tr><tr class="even"><td><img src="signs/pl50.png"></td><td><img src="signs/pl60.png"></td><td><img src="signs/pl65.png"></td><td><img src="signs/pl70.png"></td><td><img src="signs/pl80.png"></td><td><img src="signs/pl90.png"></td><td><img src="signs/pl100.png"></td><td><img src="signs/pl110.png"></td><td><img src="signs/pl120.png"></td><td><img src="signs/plx.png"></td></tr><tr class="odd"><td>101 pl50</td><td>102 pl60</td><td>103 pl65</td><td>104 pl70</td><td>105 pl80</td><td>106 pl90</td><td>107 pl100</td><td>108 pl110</td><td>109 pl120</td><td>110 plx</td></tr><tr class="even"><td><img src="signs/pm1.5.png"></td><td><img src="signs/pm2.png"></td><td><img src="signs/pm2.5.png"></td><td><img src="signs/pm5.png"></td><td><img src="signs/pm8.png"></td><td><img src="signs/pm10.png"></td><td><img src="signs/pm13.png"></td><td><img src="signs/pm15.png"></td><td><img src="signs/pm20.png"></td><td><img src="signs/pm25.png"></td></tr><tr class="odd"><td>111 pm1.5</td><td>112 pm2</td><td>113 pm2.5</td><td>114 pm5</td><td>115 pm8</td><td>116 pm10</td><td>117 pm13</td><td>118 pm15</td><td>119 pm20</td><td>120 pm25</td></tr><tr class="even"><td><img src="signs/pm30.png"></td><td><img src="signs/pm35.png"></td><td><img src="signs/pm40.png"></td><td><img src="signs/pm46.png"></td><td><img src="signs/pm50.png"></td><td><img src="signs/pm55.png"></td><td><img src="signs/pmx.png"></td><td><img src="signs/pn.png"></td><td><img src="signs/pn40.png"></td><td><img src="signs/pne.png"></td></tr><tr class="odd"><td>121 pm30</td><td>122 pm35</td><td>123 pm40</td><td>124 pm46</td><td>125 pm50</td><td>126 pm55</td><td>127 pmx</td><td>128 pn</td><td>129 pn40</td><td>130 pne</td></tr><tr class="even"><td><img src="signs/pnl.png"></td><td><img src="signs/po.png"></td><td><img src="signs/pr10.png"></td><td><img src="signs/pr20.png"></td><td><img src="signs/pr30.png"></td><td><img src="signs/pr40.png"></td><td><img src="signs/pr45.png"></td><td><img src="signs/pr50.png"></td><td><img src="signs/pr60.png"></td><td><img src="signs/pr70.png"></td></tr><tr class="odd"><td>131 pnl</td><td>132 po</td><td>133 pr10</td><td>134 pr20</td><td>135 pr30</td><td>136 pr40</td><td>137 pr45</td><td>138 pr50</td><td>139 pr60</td><td>140 pr70</td></tr><tr class="even"><td><img src="signs/pr80.png"></td><td><img src="signs/pr100.png"></td><td><img src="signs/prx.png"></td><td><img src="signs/ps.png"></td><td><img src="signs/pw2.png"></td><td><img src="signs/pw2.5.png"></td><td><img src="signs/pw3.png"></td><td><img src="signs/pw3.2.png"></td><td><img src="signs/pw3.5.png"></td><td><img src="signs/pw4.png"></td></tr><tr class="odd"><td>141 pr80</td><td>142 pr100</td><td>143 prx</td><td>144 ps</td><td>145 pw2</td><td>146 pw2.5</td><td>147 pw3</td><td>148 pw3.2</td><td>149 pw3.5</td><td>150 pw4</td></tr><tr class="even"><td><img src="signs/pw4.2.png"></td><td><img src="signs/pw4.5.png"></td><td><img src="signs/pwx.png"></td><td><img src="signs/w1.png"></td><td><img src="signs/w2.png"></td><td><img src="signs/w3.png"></td><td><img src="signs/w4.png"></td><td><img src="signs/w5.png"></td><td><img src="signs/w6.png"></td><td><img src="signs/w7.png"></td></tr><tr class="odd"><td>151 pw4.2</td><td>152 pw4.5</td><td>153 pwx</td><td>154 w1</td><td>155 w2</td><td>156 w3</td><td>157 w4</td><td>158 w5</td><td>159 w6</td><td>160 w7</td></tr><tr class="even"><td><img src="signs/w8.png"></td><td><img src="signs/w9.png"></td><td><img src="signs/w10.png"></td><td><img src="signs/w11.png"></td><td><img src="signs/w12.png"></td><td><img src="signs/w13.png"></td><td><img src="signs/w14.png"></td><td><img src="signs/w15.png"></td><td><img src="signs/w16.png"></td><td><img src="signs/w17.png"></td></tr><tr class="odd"><td>161 w8</td><td>162 w9</td><td>163 w10</td><td>164 w11</td><td>165 w12</td><td>166 w13</td><td>167 w14</td><td>168 w15</td><td>169 w16</td><td>170 w17</td></tr><tr class="even"><td><img src="signs/w18.png"></td><td><img src="signs/w19.png"></td><td><img src="signs/w20.png"></td><td><img src="signs/w21.png"></td><td><img src="signs/w22.png"></td><td><img src="signs/w23.png"></td><td><img src="signs/w24.png"></td><td><img src="signs/w25.png"></td><td><img src="signs/w26.png"></td><td><img src="signs/w27.png"></td></tr><tr class="odd"><td>171 w18</td><td>172 w19</td><td>173 w20</td><td>174 w21</td><td>175 w22</td><td>176 w23</td><td>177 w24</td><td>178 w25</td><td>179 w26</td><td>180 w27</td></tr><tr class="even"><td><img src="signs/w28.png"></td><td><img src="signs/w29.png"></td><td><img src="signs/w30.png"></td><td><img src="signs/w31.png"></td><td><img src="signs/w32.png"></td><td><img src="signs/w33.png"></td><td><img src="signs/w34.png"></td><td><img src="signs/w35.png"></td><td><img src="signs/w36.png"></td><td><img src="signs/w37.png"></td></tr><tr class="odd"><td>181 w28</td><td>182 w29</td><td>183 w30</td><td>184 w31</td><td>185 w32</td><td>186 w33</td><td>187 w34</td><td>188 w35</td><td>189 w36</td><td>190 w37</td></tr><tr class="even"><td><img src="signs/w38.png"></td><td><img src="signs/w39.png"></td><td><img src="signs/w40.png"></td><td><img src="signs/w41.png"></td><td><img src="signs/w42.png"></td><td><img src="signs/w43.png"></td><td><img src="signs/w44.png"></td><td><img src="signs/w45.png"></td><td><img src="signs/w46.png"></td><td><img src="signs/w47.png"></td></tr><tr class="odd"><td>191 w38</td><td>192 w39</td><td>193 w40</td><td>194 w41</td><td>195 w42</td><td>196 w43</td><td>197 w44</td><td>198 w45</td><td>199 w46</td><td>200 w47</td></tr><tr class="even"><td><img src="signs/w48.png"></td><td><img src="signs/w49.png"></td><td><img src="signs/w50.png"></td><td><img src="signs/w51.png"></td><td><img src="signs/w52.png"></td><td><img src="signs/w53.png"></td><td><img src="signs/w54.png"></td><td><img src="signs/w55.png"></td><td><img src="signs/w56.png"></td><td><img src="signs/w57.png"></td></tr><tr class="odd"><td>201 w48</td><td>202 w49</td><td>203 w50</td><td>204 w51</td><td>205 w52</td><td>206 w53</td><td>207 w54</td><td>208 w55</td><td>209 w56</td><td>210 w57</td></tr><tr class="even"><td><img src="signs/w58.png"></td><td><img src="signs/w59.png"></td><td><img src="signs/w60.png"></td><td><img src="signs/w61.png"></td><td><img src="signs/w62.png"></td><td><img src="signs/w63.png"></td><td><img src="signs/w64.png"></td><td><img src="signs/w65.png"></td><td><img src="signs/w66.png"></td><td><img src="signs/w67.png"></td></tr><tr class="odd"><td>211 w58</td><td>212 w59</td><td>213 w60</td><td>214 w61</td><td>215 w62</td><td>216 w63</td><td>217 w64</td><td>218 w65</td><td>219 w66</td><td>220 w67</td></tr><tr class="even"><td><img src="signs/wo.png"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td>221 wo</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="train">train</h3><p>train: 7196 samples (train 6105 samples + other 7641 samples - 6550 negative samples)</p><p>7196 images, 18159 bboxes</p><p>count_images_for_bbox:</p><table><thead><tr class="header"><th>count_bbox</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th></tr></thead><tbody><tr class="odd"><td>count_images</td><td>2745</td><td>1922</td><td>958</td><td>568</td><td>441</td><td>234</td><td>83</td><td>85</td><td>104</td><td>21</td><td>13</td><td>21</td><td>1</td></tr></tbody></table><p>count_bbox_min_size:</p><table><thead><tr class="header"><th>bbox_min_size</th><th>0</th><th>20</th><th>40</th><th>60</th><th>80</th><th>100</th><th>120</th><th>140</th><th>160</th><th>180</th><th>200</th><th>220</th><th>240</th><th>260</th><th>280</th><th>360</th></tr></thead><tbody><tr class="odd"><td>count</td><td>144</td><td>7737</td><td>5079</td><td>2531</td><td>1385</td><td>673</td><td>312</td><td>144</td><td>86</td><td>35</td><td>18</td><td>11</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><p>average bbox_min_size: 42.3916876326</p><p>count_bbox_height_width_ratio:</p><table><thead><tr class="header"><th>bbox_height_width_ratio</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th></tr></thead><tbody><tr class="odd"><td>count</td><td>5</td><td>16655</td><td>1353</td><td>116</td><td>19</td><td>9</td><td>1</td><td>1</td></tr></tbody></table><p>average bbox_height_width_ratio: 1.14734565069</p><p>count_images_for_category_id:</p><table><thead><tr class="header"><th>category_id</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th><th>17</th><th>18</th><th>19</th><th>20</th></tr></thead><tbody><tr class="odd"><td>count_images</td><td>6</td><td>292</td><td>6</td><td>474</td><td>1077</td><td>0</td><td>0</td><td>0</td><td>0</td><td>54</td><td>1</td><td>12</td><td>13</td><td>4</td><td>3</td><td>15</td><td>254</td><td>9</td><td>176</td><td>57</td></tr><tr class="even"><td>category_id</td><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td><td>26</td><td>27</td><td>28</td><td>29</td><td>30</td><td>31</td><td>32</td><td>33</td><td>34</td><td>35</td><td>36</td><td>37</td><td>38</td><td>39</td><td>40</td></tr><tr class="odd"><td>count_images</td><td>93</td><td>19</td><td>0</td><td>371</td><td>172</td><td>55</td><td>11</td><td>112</td><td>2</td><td>269</td><td>69</td><td>1</td><td>9</td><td>47</td><td>235</td><td>978</td><td>111</td><td>4</td><td>29</td><td>4</td></tr><tr class="even"><td>category_id</td><td>41</td><td>42</td><td>43</td><td>44</td><td>45</td><td>46</td><td>47</td><td>48</td><td>49</td><td>50</td><td>51</td><td>52</td><td>53</td><td>54</td><td>55</td><td>56</td><td>57</td><td>58</td><td>59</td><td>60</td></tr><tr class="odd"><td>count_images</td><td>14</td><td>23</td><td>28</td><td>89</td><td>2</td><td>2</td><td>31</td><td>162</td><td>1</td><td>37</td><td>485</td><td>84</td><td>2</td><td>0</td><td>1</td><td>11</td><td>2</td><td>14</td><td>47</td><td>0</td></tr><tr class="even"><td>category_id</td><td>61</td><td>62</td><td>63</td><td>64</td><td>65</td><td>66</td><td>67</td><td>68</td><td>69</td><td>70</td><td>71</td><td>72</td><td>73</td><td>74</td><td>75</td><td>76</td><td>77</td><td>78</td><td>79</td><td>80</td></tr><tr class="odd"><td>count_images</td><td>47</td><td>2</td><td>0</td><td>0</td><td>104</td><td>2</td><td>10</td><td>1</td><td>7</td><td>2</td><td>6</td><td>3</td><td>15</td><td>2</td><td>12</td><td>1</td><td>20</td><td>5</td><td>0</td><td>80</td></tr><tr class="even"><td>category_id</td><td>81</td><td>82</td><td>83</td><td>84</td><td>85</td><td>86</td><td>87</td><td>88</td><td>89</td><td>90</td><td>91</td><td>92</td><td>93</td><td>94</td><td>95</td><td>96</td><td>97</td><td>98</td><td>99</td><td>100</td></tr><tr class="odd"><td>count_images</td><td>21</td><td>4</td><td>1</td><td>112</td><td>8</td><td>66</td><td>4</td><td>1</td><td>0</td><td>3</td><td>0</td><td>1</td><td>223</td><td>23</td><td>72</td><td>97</td><td>6</td><td>383</td><td>20</td><td>880</td></tr><tr class="even"><td>category_id</td><td>101</td><td>102</td><td>103</td><td>104</td><td>105</td><td>106</td><td>107</td><td>108</td><td>109</td><td>110</td><td>111</td><td>112</td><td>113</td><td>114</td><td>115</td><td>116</td><td>117</td><td>118</td><td>119</td><td>120</td></tr><tr class="odd"><td>count_images</td><td>681</td><td>518</td><td>1</td><td>100</td><td>542</td><td>44</td><td>269</td><td>39</td><td>159</td><td>0</td><td>2</td><td>5</td><td>0</td><td>13</td><td>8</td><td>47</td><td>2</td><td>25</td><td>105</td><td>9</td></tr><tr class="even"><td>category_id</td><td>121</td><td>122</td><td>123</td><td>124</td><td>125</td><td>126</td><td>127</td><td>128</td><td>129</td><td>130</td><td>131</td><td>132</td><td>133</td><td>134</td><td>135</td><td>136</td><td>137</td><td>138</td><td>139</td><td>140</td></tr><tr class="odd"><td>count_images</td><td>72</td><td>4</td><td>7</td><td>6</td><td>9</td><td>95</td><td>0</td><td>1934</td><td>1</td><td>1415</td><td>0</td><td>549</td><td>1</td><td>27</td><td>47</td><td>136</td><td>0</td><td>30</td><td>42</td><td>16</td></tr><tr class="even"><td>category_id</td><td>141</td><td>142</td><td>143</td><td>144</td><td>145</td><td>146</td><td>147</td><td>148</td><td>149</td><td>150</td><td>151</td><td>152</td><td>153</td><td>154</td><td>155</td><td>156</td><td>157</td><td>158</td><td>159</td><td>160</td></tr><tr class="odd"><td>count_images</td><td>9</td><td>2</td><td>0</td><td>62</td><td>1</td><td>1</td><td>3</td><td>4</td><td>1</td><td>4</td><td>1</td><td>3</td><td>0</td><td>1</td><td>0</td><td>11</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr class="even"><td>category_id</td><td>161</td><td>162</td><td>163</td><td>164</td><td>165</td><td>166</td><td>167</td><td>168</td><td>169</td><td>170</td><td>171</td><td>172</td><td>173</td><td>174</td><td>175</td><td>176</td><td>177</td><td>178</td><td>179</td><td>180</td></tr><tr class="odd"><td>count_images</td><td>7</td><td>0</td><td>4</td><td>0</td><td>1</td><td>98</td><td>0</td><td>20</td><td>32</td><td>0</td><td>10</td><td>0</td><td>11</td><td>46</td><td>46</td><td>0</td><td>12</td><td>0</td><td>9</td><td>0</td></tr><tr class="even"><td>category_id</td><td>181</td><td>182</td><td>183</td><td>184</td><td>185</td><td>186</td><td>187</td><td>188</td><td>189</td><td>190</td><td>191</td><td>192</td><td>193</td><td>194</td><td>195</td><td>196</td><td>197</td><td>198</td><td>199</td><td>200</td></tr><tr class="odd"><td>count_images</td><td>0</td><td>0</td><td>72</td><td>3</td><td>66</td><td>0</td><td>12</td><td>2</td><td>0</td><td>4</td><td>3</td><td>0</td><td>0</td><td>7</td><td>26</td><td>6</td><td>1</td><td>16</td><td>5</td><td>35</td></tr><tr class="even"><td>category_id</td><td>201</td><td>202</td><td>203</td><td>204</td><td>205</td><td>206</td><td>207</td><td>208</td><td>209</td><td>210</td><td>211</td><td>212</td><td>213</td><td>214</td><td>215</td><td>216</td><td>217</td><td>218</td><td>219</td><td>220</td></tr><tr class="odd"><td>count_images</td><td>1</td><td>1</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>109</td><td>0</td><td>252</td><td>49</td><td>124</td><td>1</td><td>0</td><td>1</td><td>73</td><td>0</td><td>0</td><td>4</td><td>0</td></tr><tr class="even"><td>category_id</td><td>221</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td>count_images</td><td>60</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>avg 75.1990950226 max 1934 (1) min 0 (46)</p><h3 id="test">test</h3><p>test: 3071 samples</p><p>3071 images, 8190 bboxes</p><p>count_images_for_bbox:</p><table><thead><tr class="header"><th>count_bbox</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>18</th></tr></thead><tbody><tr class="odd"><td>count_images</td><td>1042</td><td>844</td><td>407</td><td>278</td><td>236</td><td>111</td><td>53</td><td>39</td><td>42</td><td>6</td><td>4</td><td>6</td><td>2</td><td>1</td></tr></tbody></table><p>count_bbox_min_size:</p><table><thead><tr class="header"><th>bbox_min_size</th><th>0</th><th>20</th><th>40</th><th>60</th><th>80</th><th>100</th><th>120</th><th>140</th><th>160</th><th>180</th><th>200</th><th>220</th><th>240</th><th>260</th><th>300</th><th>320</th><th>340</th><th>400</th></tr></thead><tbody><tr class="odd"><td>count</td><td>16</td><td>3440</td><td>2459</td><td>1107</td><td>561</td><td>306</td><td>139</td><td>77</td><td>44</td><td>23</td><td>7</td><td>2</td><td>2</td><td>2</td><td>2</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><p>average bbox_min_size: 42.8355321502</p><p>count_bbox_height_width_ratio:</p><table><thead><tr class="header"><th>bbox_height_width_ratio</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr class="odd"><td>count</td><td>8</td><td>7402</td><td>699</td><td>69</td><td>10</td><td>2</td></tr></tbody></table><p>average bbox_height_width_ratio: 1.15697810748</p><p>count_images_for_category_id:</p><table><thead><tr class="header"><th>category_id</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th><th>17</th><th>18</th><th>19</th><th>20</th></tr></thead><tbody><tr class="odd"><td>count_images</td><td>1</td><td>140</td><td>2</td><td>223</td><td>461</td><td>0</td><td>0</td><td>0</td><td>0</td><td>15</td><td>2</td><td>1</td><td>4</td><td>1</td><td>3</td><td>3</td><td>103</td><td>5</td><td>84</td><td>17</td></tr><tr class="even"><td>category_id</td><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td><td>26</td><td>27</td><td>28</td><td>29</td><td>30</td><td>31</td><td>32</td><td>33</td><td>34</td><td>35</td><td>36</td><td>37</td><td>38</td><td>39</td><td>40</td></tr><tr class="odd"><td>count_images</td><td>39</td><td>2</td><td>0</td><td>175</td><td>114</td><td>15</td><td>5</td><td>58</td><td>2</td><td>117</td><td>39</td><td>0</td><td>4</td><td>14</td><td>86</td><td>491</td><td>66</td><td>2</td><td>13</td><td>1</td></tr><tr class="even"><td>category_id</td><td>41</td><td>42</td><td>43</td><td>44</td><td>45</td><td>46</td><td>47</td><td>48</td><td>49</td><td>50</td><td>51</td><td>52</td><td>53</td><td>54</td><td>55</td><td>56</td><td>57</td><td>58</td><td>59</td><td>60</td></tr><tr class="odd"><td>count_images</td><td>6</td><td>9</td><td>12</td><td>33</td><td>0</td><td>0</td><td>8</td><td>99</td><td>0</td><td>12</td><td>230</td><td>47</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>11</td><td>13</td><td>0</td></tr><tr class="even"><td>category_id</td><td>61</td><td>62</td><td>63</td><td>64</td><td>65</td><td>66</td><td>67</td><td>68</td><td>69</td><td>70</td><td>71</td><td>72</td><td>73</td><td>74</td><td>75</td><td>76</td><td>77</td><td>78</td><td>79</td><td>80</td></tr><tr class="odd"><td>count_images</td><td>18</td><td>0</td><td>0</td><td>0</td><td>44</td><td>0</td><td>2</td><td>0</td><td>5</td><td>1</td><td>2</td><td>0</td><td>0</td><td>0</td><td>4</td><td>1</td><td>0</td><td>4</td><td>1</td><td>36</td></tr><tr class="even"><td>category_id</td><td>81</td><td>82</td><td>83</td><td>84</td><td>85</td><td>86</td><td>87</td><td>88</td><td>89</td><td>90</td><td>91</td><td>92</td><td>93</td><td>94</td><td>95</td><td>96</td><td>97</td><td>98</td><td>99</td><td>100</td></tr><tr class="odd"><td>count_images</td><td>8</td><td>2</td><td>0</td><td>55</td><td>3</td><td>30</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>130</td><td>12</td><td>17</td><td>55</td><td>2</td><td>202</td><td>1</td><td>432</td></tr><tr class="even"><td>category_id</td><td>101</td><td>102</td><td>103</td><td>104</td><td>105</td><td>106</td><td>107</td><td>108</td><td>109</td><td>110</td><td>111</td><td>112</td><td>113</td><td>114</td><td>115</td><td>116</td><td>117</td><td>118</td><td>119</td><td>120</td></tr><tr class="odd"><td>count_images</td><td>338</td><td>262</td><td>0</td><td>44</td><td>259</td><td>3</td><td>124</td><td>3</td><td>79</td><td>0</td><td>0</td><td>2</td><td>1</td><td>2</td><td>1</td><td>1</td><td>0</td><td>5</td><td>47</td><td>0</td></tr><tr class="even"><td>category_id</td><td>121</td><td>122</td><td>123</td><td>124</td><td>125</td><td>126</td><td>127</td><td>128</td><td>129</td><td>130</td><td>131</td><td>132</td><td>133</td><td>134</td><td>135</td><td>136</td><td>137</td><td>138</td><td>139</td><td>140</td></tr><tr class="odd"><td>count_images</td><td>31</td><td>1</td><td>1</td><td>0</td><td>2</td><td>37</td><td>0</td><td>913</td><td>0</td><td>615</td><td>0</td><td>282</td><td>0</td><td>5</td><td>2</td><td>63</td><td>2</td><td>6</td><td>10</td><td>2</td></tr><tr class="even"><td>category_id</td><td>141</td><td>142</td><td>143</td><td>144</td><td>145</td><td>146</td><td>147</td><td>148</td><td>149</td><td>150</td><td>151</td><td>152</td><td>153</td><td>154</td><td>155</td><td>156</td><td>157</td><td>158</td><td>159</td><td>160</td></tr><tr class="odd"><td>count_images</td><td>2</td><td>0</td><td>0</td><td>13</td><td>0</td><td>0</td><td>1</td><td>7</td><td>1</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr class="even"><td>category_id</td><td>161</td><td>162</td><td>163</td><td>164</td><td>165</td><td>166</td><td>167</td><td>168</td><td>169</td><td>170</td><td>171</td><td>172</td><td>173</td><td>174</td><td>175</td><td>176</td><td>177</td><td>178</td><td>179</td><td>180</td></tr><tr class="odd"><td>count_images</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>31</td><td>0</td><td>2</td><td>3</td><td>0</td><td>3</td><td>0</td><td>3</td><td>9</td><td>14</td><td>0</td><td>3</td><td>0</td><td>0</td><td>0</td></tr><tr class="even"><td>category_id</td><td>181</td><td>182</td><td>183</td><td>184</td><td>185</td><td>186</td><td>187</td><td>188</td><td>189</td><td>190</td><td>191</td><td>192</td><td>193</td><td>194</td><td>195</td><td>196</td><td>197</td><td>198</td><td>199</td><td>200</td></tr><tr class="odd"><td>count_images</td><td>1</td><td>0</td><td>20</td><td>0</td><td>33</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>4</td><td>3</td><td>7</td></tr><tr class="even"><td>category_id</td><td>201</td><td>202</td><td>203</td><td>204</td><td>205</td><td>206</td><td>207</td><td>208</td><td>209</td><td>210</td><td>211</td><td>212</td><td>213</td><td>214</td><td>215</td><td>216</td><td>217</td><td>218</td><td>219</td><td>220</td></tr><tr class="odd"><td>count_images</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>59</td><td>1</td><td>115</td><td>19</td><td>60</td><td>0</td><td>0</td><td>0</td><td>13</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr class="even"><td>category_id</td><td>221</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td>count_images</td><td>34</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>avg 33.8280542986 max 913 (1) min 0 (85)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(CVPR 2016) Traffic-Sign Detection and Classification in the Wild&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Page: &lt;a href=&quot;https://cg.cs.tsinghua.edu.cn/traffic-sign/&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cg.cs.tsinghua.edu.cn/traffic-sign/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/code.zip&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/code.zip&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="Paper" scheme="http://lijiancheng0614.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>ENAS</title>
    <link href="http://lijiancheng0614.github.io/2018/10/31/2018_10_31_ENAS/"/>
    <id>http://lijiancheng0614.github.io/2018/10/31/2018_10_31_ENAS/</id>
    <published>2018-10-30T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.812Z</updated>
    
    <content type="html"><![CDATA[<p>(ICML 2018) Efficient Neural Architecture Search via Parameter Sharing</p><p>Paper: <a href="https://arxiv.org/abs/1802.03268" class="uri" target="_blank" rel="noopener">https://arxiv.org/abs/1802.03268</a></p><p>Code: <a href="https://github.com/melodyguan/enas" class="uri" target="_blank" rel="noopener">https://github.com/melodyguan/enas</a></p><a id="more"></a><p>我们提出了高效的神经架构搜索（ENAS），一种快速且廉价的自动模型设计方法。在 ENAS 中，有一个控制器通过在一个大型计算图中搜索一个最优的子图以学习发现最优神经网络架构的方法。控制器采用策略梯度进行训练，以选择最大化验证集期望奖励的子图。同时，和所选子图对应的模型将进行训练以最小化标准交叉熵损失。由于子模型之间的参数共享，ENAS 的速度很快：它只需要使用少得多的 GPU 运算时间就能达到比当前的自动化模型设计方法好很多的经验性能，尤其是，其计算成本只有标准的神经架构搜索（NAS）的千分之一。</p><p>We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller discovers neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on a validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, while using much fewer GPUhours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search.</p><h1 id="中文">中文</h1><h2 id="简介">简介</h2><p>本文研究做出的主要贡献是通过强制所有子模型共享权重而提升了 NAS 的效率。</p><p>重要的是，在本研究所有使用单个 Nvidia GTX 1080Ti GPU 的实验中，搜索架构的时间都少于 16 小时。相较于 NAS，GPU 运算时间缩短了 1000 倍以上。</p><h2 id="方法">方法</h2><p>ENAS 思想的核心是观察到 NAS 最终迭代的所有图可以看作更大图的子图。换句话说，我们可以使用单个有向无环图（DAG）来表征 NAS 的搜索空间。</p><h3 id="设计循环单元">设计循环单元</h3><p>为了设计循环单元，作者使用了有 N 个结点的有向无环图（DAG），其中每个节点代表局部运算，而每条边代表 N 个节点中的信息流。</p><p>ENAS 的控制器是一个 RNN，它会控制：</p><ol type="1"><li><p>哪一条边处于激活状态</p></li><li><p>在 DAG 中的每一个结点会执行哪些运算</p></li></ol><p><img src="figure1.png"></p><p>图 1：搜索空间中带有四个计算节点的循环单元案例。左图为对应循环单元的计算 DAG，其中红色的边代表图中的信息流。中间为循环单元。右图为 RNN 控制器的输出结果，它将会生成中间的循环单元和左边的 DAG。注意节点 3 和 4 永远不会被 RNN 采样，所以它们结果是平均值，且可以作为单元的输出。</p><h3 id="训练-enas-及衍生架构">训练 ENAS 及衍生架构</h3><p>我们的控制器网络是一个有 100 个隐藏单元的LSTM。</p><p>在 ENAS 中，有两组可学习的参数：控制器 LSTM 的参数，由 <span class="math inline">\(\theta\)</span> 表示，以及子模型的共享参数，由 <span class="math inline">\(\omega\)</span> 表示。</p><p>第一阶段训练 <span class="math inline">\(\omega\)</span>，子模型的共享参数，通过训练数据集训练一个完整的 pass。</p><p>第二阶段训练控制器 LSTM 的参数 <span class="math inline">\(\theta\)</span>，固定步数，在我们的实验中通常设置为 2000。</p><p><strong>训练子模型的共享参数 <span class="math inline">\(\omega\)</span></strong></p><p>在此步骤中，我们修复控制器的策略 <span class="math inline">\(\pi(m; \theta)\)</span> 并在 <span class="math inline">\(\omega\)</span> 上执行随机梯度下降（SGD）最小化预期损失函数（标准交叉熵损失）。使用蒙特卡洛估计来计算梯度。</p><p><strong>训练控制器参数 <span class="math inline">\(\theta\)</span></strong></p><p>在此步骤中，我们修复 <span class="math inline">\(\omega\)</span> 并更新策略参数 <span class="math inline">\(\theta\)</span>，旨在最大化预期的奖励。使用 REINFORCE 计算梯度。</p><p><strong>推导架构</strong></p><p>我们首先从训练好的策略 <span class="math inline">\(\pi(m; \theta)\)</span> 中抽取几个模型。对于每个采样模型，我们从验证集中采样的单个小批量上计算其奖励。然后我们只采用奖励最高的模型从头开始重新训练。</p><h3 id="设计卷积网络">设计卷积网络</h3><p>在卷积网络的搜索空间中，RNN 控制器在每一个决策模块上会制定两组决策：</p><ol type="1"><li><p>前一个单元需要连接什么；</p></li><li><p>它需要什么样的运算过程。</p></li></ol><p>这些决策共同构建了卷积网络中的不同层级。</p><p><img src="figure3.png"></p><p>图 3：搜索空间中一个循环单元的实例运行，该空间带有 4 个计算节点，表征卷积网络的 4 个层。顶部：控制器 RNN 的输出。左下：对应于网络架构的计算 DAG。红箭头表征激活的计算路径。右下：完整的网络。虚线箭头表征跳跃连接。</p><p>控制器可用的6种操作是：滤波器大小为 3 x 3 和 5 x 5 的卷积，滤波器大小为 3 x 3 和 5 x 5 的深度可分离卷积，核大小为 3 x 3 的最大池化和平均池化。</p><h3 id="设计卷积单元">设计卷积单元</h3><p>本文并没有采用直接设计完整的卷积网络的方法，而是先设计小型的模块然后将模块连接以构建完整的网络。图 4 展示了这种设计的例子，其中设计了卷积单元和 reduction cell。</p><p><img src="figure4.png"></p><p>图 4：连接卷积单元和 reduction 单元，以构建完整的网络。</p><p>5 个可用的操作是：identity，核大小为 3 x 3 和 5 x 5 的可分离卷积，核大小为 3 x 3 的平均池化和最大池化。</p><p>还可以从我们讨论的搜索空间中实现缩小单元，简单地通过以下方式：</p><ol type="1"><li>从搜索空间中对计算图进行采样</li><li>对所有操作设置步长为 2</li></ol><p>因此，缩小单元减小了其空间维度，为原来的一半。</p><h2 id="实验">实验</h2><h3 id="在-penn-treebank-数据集上训练的语言模型">在 Penn Treebank 数据集上训练的语言模型</h3><p><img src="figure6.png"></p><p>图 6：ENAS 为 Penn Treebank 数据集发现的 RNN 单元。</p><p><img src="table1.png"></p><p>表 1：ENAS 为 Penn Treebank 数据集发现的架构的测试困惑度以及其它的基线结果的比较。（缩写说明：RHN 是 Recurrent Highway Network、VD 是 Variational Dropout、WT 是 Weight Tying、ℓ2 是 Weight Penalty、AWD 是 Averaged Weight Drop、MoC 是 Mixture of Contexts、MoS 是 Mixture of Softmaxes。）</p><h3 id="在-cifar-10-数据集上的图像分类实验">在 CIFAR-10 数据集上的图像分类实验</h3><p><img src="table2.png"></p><p>表 2：ENAS 发现的架构在 CIFAR-10 数据集上的分类误差和其它基线结果的对比。在这个表中，第一块展示了 DenseNet，由人类专家设计的当前最佳架构。第二块展示了设计整个网络的方法。最后一块展示了设计模块单元以构建大型模型的技术。</p><p><img src="figure7.png"></p><p>图 7：ENAS 从大型搜索空间中发现的用于图像分类的网络架构。</p><p>ENAS 用了 11.5 个小时来发现合适的卷积单元和 reduction 单元，如图 8 所示。</p><p><img src="figure8.png"></p><p>图 8：ENAS 在微搜索空间中挖掘新的单元。</p><h3 id="enas-的重要性">ENAS 的重要性</h3><h2 id="相关工作和讨论">相关工作和讨论</h2><p>##结论</p><p>在本文中，我们介绍了 ENAS，一种在 GPU 小时数方面加速 NAS 超过1000倍的新方法。ENAS的关键贡献是在搜索体系结构时跨子模型共享参数。</p><h1 id="english">English</h1><h2 id="introduction">Introduction</h2><p>The main contribution of this work is to improve the efficiency of NAS by forcing all child models to share weights to eschew training each child model from scratch to convergence.</p><p>Importantly, in all of our experiments, for which we use a single Nvidia GTX 1080Ti GPU, the search for architectures takes less than 16 hours. Compared to NAS, this is a reduction of GPU-hours by more than 1000x.</p><h2 id="methods">Methods</h2><p>Central to the idea of ENAS is the observation that all of the graphs which NAS ends up iterating over can be viewed as sub-graphs of a larger graph. In other words, we can represent NAS’s search space using a single directed acyclic graph (DAG).</p><h3 id="designing-recurrent-cells">Designing Recurrent Cells</h3><p>To design recurrent cells, we employ a DAG with N nodes, where the nodes represent local computations, and the edges represent the flow of information between the N nodes.</p><p>ENAS’s controller is an RNN that decides:</p><ol type="1"><li><p>which edges are activated</p></li><li><p>which computations are performed at each node in the DAG.</p></li></ol><p><img src="figure1.png"></p><p>Figure 1. An example of a recurrent cell in our search space with 4 computational nodes. Left: The computational DAG that corresponds to the recurrent cell. The red edges represent the flow of information in the graph. Middle: The recurrent cell. Right: The outputs of the controller RNN that result in the cell in the middle and the DAG on the left. Note that nodes 3 and 4 are never sampled by the RNN, so their results are averaged and are treated as the cell’s output.</p><h3 id="training-enas-and-deriving-architectures">Training ENAS and Deriving Architectures</h3><p>Our controller network is an LSTM with 100 hidden units.</p><p>In ENAS, there are two sets of learnable parameters: the parameters of the controller LSTM, denoted by <span class="math inline">\(\theta\)</span>, and the shared parameters of the child models, denoted by <span class="math inline">\(\omega\)</span>.</p><p>The first phase trains <span class="math inline">\(\omega\)</span>, the shared parameters of the child models, on a whole pass through the training data set.</p><p>The second phase trains <span class="math inline">\(\theta\)</span>, the parameters of the controller LSTM, for a fixed number of steps, typically set to 2000 in our experiments.</p><p><strong>Training the shared parameters <span class="math inline">\(\omega\)</span> of the child models.</strong></p><p>In this step, we fix the controller’s policy <span class="math inline">\(\pi(m; \theta)\)</span> and perform stochastic gradient descent (SGD) on <span class="math inline">\(\omega\)</span> to minimize the expected loss function (the standard cross-entropy loss). The gradient is computed using the Monte Carlo estimate.</p><p><strong>Training the controller parameters <span class="math inline">\(\theta\)</span>.</strong></p><p>In this step, we fix <span class="math inline">\(\omega\)</span> and update the policy parameters <span class="math inline">\(\theta\)</span>, aiming to maximize the expected reward. The gradient is computed using REINFORCE.</p><p><strong>Deriving Architectures.</strong></p><p>We first sample several models from the trained policy <span class="math inline">\(\pi(m; \theta)\)</span>. For each sampled model, we compute its reward on a single minibatch sampled from the validation set. We then take only the model with the highest reward to re-train from scratch.</p><h3 id="designing-convolutional-networks">Designing Convolutional Networks</h3><p>In the search space for convolutional models, the controller RNN also samples two sets of decisions at each decision block:</p><ol type="1"><li><p>what previous nodes to connect to</p></li><li><p>what computation operation to use.</p></li></ol><p>These decisions construct a layer in the convolutional model.</p><p><img src="figure3.png"></p><p>Figure 3. An example run of a recurrent cell in our search space with 4 computational nodes, which represent 4 layers in a convolutional network. Top: The output of the controller RNN. Bot- tom Left: The computational DAG corresponding to the network’s architecture. Red arrows denote the active computational paths. Bottom Right: The complete network. Dotted arrows denote skip connections.</p><p>The 6 operations available for the controller are: convolutions with filter sizes 3 x 3 and 5 x 5, depthwise-separable convolutionswith filter sizes 3 x 3 and 5 x 5, and max pooling and average pooling of kernel size 3 x 3.</p><h3 id="designing-convolutional-cells">Designing Convolutional Cells</h3><p>Rather than designing the entire convolutional network, one can design smaller modules and then connect them together to form a network. Figure 4 illustrates this design, where the convolutional cell and reduction cell architectures are to be designed.</p><p><img src="figure4.png"></p><p>Figure 4. Connecting 3 blocks, each with N convolution cells and 1 reduction cell, to make the final network.</p><p>The 5 available operations are: identity, separable convolution with kernel size 3 x 3 and 5 x 5, and average pooling and max pooling with kernel size 3 x 3.</p><p>A reduction cell can also be realized from the search space we discussed, simply by:</p><ol type="1"><li><p>sampling a computational graph from the search space</p></li><li><p>applying all operations with a stride of 2.</p></li></ol><p>A reduction cell thus reduces the spatial dimensions of its input by a factor of 2.</p><h2 id="experiments">Experiments</h2><h3 id="language-model-with-penn-treebank">Language Model with Penn Treebank</h3><p><img src="figure6.png"></p><p>Figure 6. The RNN cell ENAS discovered for Penn Treebank.</p><p><img src="table1.png"></p><p>Table 1. Test perplexity on Penn Treebank of ENAS and other baselines. Abbreviations: RHN is Recurrent Highway Network, VD is Variational Dropout; WT is Weight Tying; ℓ2 is Weight Penalty; AWD is Averaged Weight Drop; MoC is Mixture of Contexts; MoS is Mixture of Softmaxes.</p><h3 id="image-classification-on-cifar-10">Image Classification on CIFAR-10</h3><p><img src="table2.png"></p><p>Table 2. Classification errors of ENAS and baselines on CIFAR-10. In this table, the first block presents DenseNet, one of the state-ofthe- art architectures designed by human experts. The second block presents approaches that design the entire network. The last block presents techniques that design modular cells which are combined to build the final network.</p><p><img src="figure7.png"></p><p>Figure 7. ENAS’s discovered network from the macro search space for image classification.</p><p>ENAS takes 11.5 hours to discover the convolution cell and the reduction cell, which are visualized in Figure 8.</p><p><img src="figure8.png"></p><p>Figure 8. ENAS cells discovered in the micro search space.</p><h3 id="the-importance-of-enas">The Importance of ENAS</h3><h2 id="related-work-and-discussions">Related Work and Discussions</h2><h2 id="conclusion">Conclusion</h2><p>In this paper, we presented ENAS, a novel method that speeds up NAS by more than 1000x, in terms of GPU hours. ENAS’s key contribution is the sharing of parameters across child models during the search for architectures.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(ICML 2018) Efficient Neural Architecture Search via Parameter Sharing&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1802.03268&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1802.03268&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;https://github.com/melodyguan/enas&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/melodyguan/enas&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
      <category term="Paper" scheme="http://lijiancheng0614.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>NAS</title>
    <link href="http://lijiancheng0614.github.io/2018/10/29/2018_10_29_NAS/"/>
    <id>http://lijiancheng0614.github.io/2018/10/29/2018_10_29_NAS/</id>
    <published>2018-10-28T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.809Z</updated>
    
    <content type="html"><![CDATA[<p>(ICLR 2017) Neural Architecture Search with Reinforcement Learning</p><p>Paper: <a href="https://arxiv.org/abs/1611.01578" class="uri" target="_blank" rel="noopener">https://arxiv.org/abs/1611.01578</a></p><p>Page: <a href="https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html" class="uri" target="_blank" rel="noopener">https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html</a></p><a id="more"></a><p>我们使用循环网络生成神经网络的模型描述，并使用强化学习训练此 RNN，以最大化生成的体系结构在验证集上的预期准确率。</p><p>we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.</p><h1 id="中文">中文</h1><h2 id="简介">简介</h2><p>本文介绍了神经网络搜索，这是一种基于梯度的方法，用于寻找良好的架构（参见图1）。</p><p><img src="figure1.png"></p><p>图 1：神经网络搜索概览</p><p>我们的工作基于以下观察：神经网络的结构和连通性通常可以由可变长度的字符串指定。因此可以使用循环网络，即控制器，来生成这样的字符串。</p><p>训练由字符串指定的网络，即“子网络”，在真实数据上得到验证集的准确率。使用此准确率作为奖励信号，我们可以计算策略梯度以更新控制器。因此，在下一次迭代中，控制器将为具有高精度的架构提供更高的概率。</p><h2 id="相关工作">相关工作</h2><h2 id="方法">方法</h2><h3 id="用循环神经网络控制器生成模型描述">用循环神经网络控制器生成模型描述</h3><p><img src="figure2.png"></p><p>图2：我们的控制器递归神经网络如何对简单的卷积网络进行采样。它预测一层滤波器高度、滤波器宽度、步幅高度、步幅宽度以及重复的滤波器数量。每个预测都由softmax分类器执行，然后作为输入馈入下一个时间步。</p><p>如果层数超过某个值，则生成架构的过程停止。</p><h3 id="使用强化训练">使用强化训练</h3><p>控制器预测的 token 列表可以被视为一个动作列表 <span class="math inline">\(a_{1:T}\)</span> 来设计子网络的体系结构。</p><p>收敛时，这个子网络将在不变的数据集上得到准确率 <span class="math inline">\(R\)</span>。我们可以使用此精度 <span class="math inline">\(R\)</span> 作为奖励信号，并使用强化学习来训练控制器。</p><p>更具体地说，为了找到最佳架构，我们要求我们的控制器最大化其预期奖励，由 <span class="math inline">\(J(\theta_c)\)</span> 表示：</p><p><span class="math display">\[J(\theta_c) = E_{P(a_{1:T};\theta_c)} [R]\]</span></p><p>由于奖励信号R是不可微分的，我们需要使用 <strong>策略梯度</strong> 方法来迭代更新 <span class="math inline">\(\theta_c\)</span>。我们使用 Williams（1992）的 REINFORCE 规则：</p><p><span class="math display">\[\nabla_{\theta_c} J(\theta_c) = \sum_{t = 1}^T E_{P(a_{1:T};\theta_c)} [\nabla_{\theta_c} \log P(a_t \mid a_{(t - 1):1};\theta_c) R]\]</span></p><h3 id="使用跳过连接和其他层类型来增加体系结构的复杂性">使用跳过连接和其他层类型来增加体系结构的复杂性</h3><p>为了使控制器能够预测像 GoogleNet 和 ResNet 这种连接，我们使用了基于注意机制的集合选择型注意。</p><p>在 <span class="math inline">\(N\)</span> 层，我们添加一个锚点，其中包含 <span class="math inline">\(N - 1\)</span> 个基于内容的 sigmoids，用于指示需要连接的先前层。 每个 sigmoid 是控制器的当前隐藏状态和先前 <span class="math inline">\(N - 1\)</span> 个锚点的先前隐藏状态的函数：</p><p><span class="math display">\[P(\text{Layer j is an input to layer i}) = \text{sigmoid}(v^T \tanh(W_{prev} * h_j + W_{curr} * h_i))\]</span></p><p>训练参数：<span class="math inline">\(W_{prev}\)</span>, <span class="math inline">\(W_{curr}\)</span>, <span class="math inline">\(v\)</span></p><p><img src="figure4.png"></p><p>图4：控制器使用锚点和集合选择注意力来形成跳过连接。</p><p>跳过连接可能导致“编译失败”，其中一个层与另一个层不兼容，或者一个层可能没有任何输入或输出。为了避免这些问题，我们采用了三种简单的技术：</p><ol type="1"><li><p>如果某层未连接到任何输入图层，则将图像作为输入层。</p></li><li><p>在最后一层，我们获取所有尚未连接的图层输出，并在将此最终隐藏状态发送给分类器之前将它们连接起来。</p></li><li><p>如果要连接的输入层具有不同的大小，我们用零填充小层，以便连接的层具有相同的大小。</p></li></ol><h3 id="生成循环单元结构">生成循环单元结构</h3><p>在第 <span class="math inline">\(t\)</span> 步，控制器需要找到 <span class="math inline">\(h_t\)</span> 的函数形式，它将 <span class="math inline">\(x_t\)</span> 和 <span class="math inline">\(h_{t-1}\)</span> 作为输入。最简单的方法是 <span class="math inline">\(h_t = \tanh(W_1 * x_t + W_2 * h_{t-1})\)</span></p><p>基本 RNN 和 LSTM 单元的计算可以概括为由多个步骤构成的树，其中 <span class="math inline">\(x_t\)</span> 和 <span class="math inline">\(h_{t-1}\)</span> 作为输入并产生 <span class="math inline">\(h_t\)</span> 作为最终输出。</p><p><img src="figure5.png"></p><p>图5：从具有两个叶节点（基数2）和一个内部节点的树构造一个循环单元的示例。</p><p>左：定义由控制器预测的计算步骤的树。</p><p>中心：控制器为树中的每个计算步骤做出的一组示例预测。</p><p>右：从控制器的示例预测构造的循环单元的计算图。</p><h2 id="实验和结果">实验和结果</h2><p>在 CIFAR-10 上，我们的目标是找到一个好的卷积结构，而在 Penn Treebank，我们的目标是找到一个好的循环单元。</p><h3 id="为-cifar-10-学习卷积结构">为 CIFAR-10 学习卷积结构</h3><p><strong>搜索空间</strong>：我们的搜索空间包括卷积、ReLU、Batch Normalization 和层间连接。对于每个卷积层，控制器 RNN 必须在 [1, 3, 5, 7] 中选择滤波器高度，在 [1, 3, 5, 7] 中选择滤波器宽度，并在 [24, 36, 48, 64] 中选择滤波器数量。对于步幅，我们执行两组实验，一组用于将步幅固定为 1，另一组用控制器在 [1, 2, 3] 中预测步幅。</p><p><strong>训练细节</strong>：控制器 RNN 是一个双层 LSTM，每层有 35 个隐藏单元。</p><p><strong>结果</strong>：在控制器训练 12,800 个架构后，我们发现该架构可以获得最佳的验证精度。然后，我们对学习率、体重衰减、batchnorm epsilon 以及衰减的 epoch 进行小网格搜索。</p><p><img src="table1.png"></p><p>表1：神经架构搜索和其他最先进的模型在 CIFAR-10 上的性能。</p><h3 id="为-penn-treebank-学习循环单元">为 Penn Treebank 学习循环单元</h3><p><strong>搜索空间</strong>：对于树中的每个节点，控制器 RNN 需要在 [add; elem mult] 中选择组合方法，在 [identity; tanh; sigmoid; relu] 中选择激活方法。</p><p><strong>训练细节</strong>: 该控制器及其训练与 CIFAR-10 实验基本相同，只是作了一些修改。</p><p><img src="table2.png"></p><p>表 2：Penn Treebank 语言建模任务测试集的单模型效果。</p><h2 id="结论">结论</h2><p>本文介绍了神经网络结构搜索，一种利用循环神经网络构造神经网络结构的方法。</p><h1 id="english">English</h1><h2 id="introduction">Introduction</h2><p>This paper presents Neural Architecture Search, a gradient-based method for finding good architectures (see Figure 1).</p><p><img src="figure1.png"></p><p>Figure 1: An overview of Neural Architecture Search.</p><p>Our work is based on the observation that the structure and connectivity of a neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network – the controller – to generate such string.</p><p>Training the network specified by the string – the “child network” – on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies.</p><h2 id="related-work">Related Work</h2><h2 id="methods">Methods</h2><h3 id="generate-model-descriptions-with-a-controller-recurrent-neural-network">Generate Model Descriptions with a Controller Recurrent Neural Network</h3><p><img src="figure2.png"></p><p>Figure 2: How our controller recurrent neural network samples a simple convolutional network. It predicts filter height, filter width, stride height, stride width, and number of filters for one layer and repeats. Every prediction is carried out by a softmax classifier and then fed into the next time step as input.</p><p>the process of generating an architecture stops if the number of layers exceeds a certain value.</p><h3 id="training-with-reinforce">Training with Reinforce</h3><p>The list of tokens that the controller predicts can be viewed as a list of actions <span class="math inline">\(a_{1:T}\)</span> to design an architecture for a child network.</p><p>At convergence, this child network will achieve an accuracy <span class="math inline">\(R\)</span> on a held-out dataset. We can use this accuracy <span class="math inline">\(R\)</span> as the reward signal and use reinforcement learning to train the controller.</p><p>More concretely, to find the optimal architecture, we ask our controller to maximize its expected reward, represented by <span class="math inline">\(J(\theta_c)\)</span>:</p><p><span class="math display">\[J(\theta_c) = E_{P(a_{1:T};\theta_c)} [R]\]</span></p><p>Since the reward signal R is non-differentiable, we need to use a <strong>policy gradient</strong> method to iteratively update <span class="math inline">\(\theta_c\)</span>. In this work, we use the REINFORCE rule from Williams (1992):</p><p><span class="math display">\[\nabla_{\theta_c} J(\theta_c) = \sum_{t = 1}^T E_{P(a_{1:T};\theta_c)} [\nabla_{\theta_c} \log P(a_t \mid a_{(t - 1):1};\theta_c) R]\]</span></p><h3 id="increase-architecture-complexity-with-skip-connections-and-other-layer-types">Increase Architecture Complexity with Skip Connections and Other Layer Types</h3><p>To enable the controller to predict such connections, we use a set-selection type attention which was built upon the attention mechanism.</p><p>At layer <span class="math inline">\(N\)</span>, we add an anchor point which has <span class="math inline">\(N - 1\)</span> content-based sigmoids to indicate the previous layers that need to be connected. Each sigmoid is a function of the current hiddenstate of the controller and the previous hiddenstates of the previous <span class="math inline">\(N - 1\)</span> anchor points:</p><p><span class="math display">\[P(\text{Layer j is an input to layer i}) = \text{sigmoid}(v^T \tanh(W_{prev} * h_j + W_{curr} * h_i))\]</span></p><p>trainable parameters: <span class="math inline">\(W_{prev}\)</span>, <span class="math inline">\(W_{curr}\)</span>, <span class="math inline">\(v\)</span></p><p><img src="figure4.png"></p><p>Figure 4: The controller uses anchor points, and set-selection attention to form skip connections.</p><p>Skip connections can cause “compilation failures” where one layer is not compatible with another layer, or one layer may not have any input or output. To circumvent these issues, we employ three simple techniques.</p><ol type="1"><li><p>if a layer is not connected to any input layer then the image is used as the input layer.</p></li><li><p>at the final layer we take all layer outputs that have not been connected and concatenate them before sending this final hiddenstate to the classifier.</p></li><li><p>if input layers to be concatenated have different sizes, we pad the small layers with zeros so that the concatenated layers have the same sizes.</p></li></ol><h3 id="generate-recurrent-cell-architectures">Generate Recurrent Cell Architectures</h3><p>At every time step <span class="math inline">\(t\)</span>, the controller needs to find a functional form for <span class="math inline">\(h_t\)</span> that takes <span class="math inline">\(x_t\)</span> and <span class="math inline">\(h_{t-1}\)</span> as inputs. The simplest way is to have <span class="math inline">\(h_t = \tanh(W_1 * x_t + W_2 * h_{t-1})\)</span></p><p>The computations for basic RNN and LSTM cells can be generalized as a tree of steps that take <span class="math inline">\(x_t\)</span> and <span class="math inline">\(h_{t-1}\)</span> as inputs and produce <span class="math inline">\(h_t\)</span> as final output.</p><p><img src="figure5.png"></p><p>Figure 5: An example of a recurrent cell constructed from a tree that has two leaf nodes (base 2) and one internal node.</p><p>Left: the tree that defines the computation steps to be predicted by controller.</p><p>Center: an example set of predictions made by the controller for each computation step in the tree.</p><p>Right: the computation graph of the recurrent cell constructed from example predictions of the controller.</p><p>according to the predictions of the controller RNN in this example, the following computation steps will occur:</p><ol type="1"><li><p>The controller predicts <em>Add</em> and <em>Tanh</em> for tree index 0, this means we need to compute <span class="math inline">\(a_0 = tanh(W_1 * x_t + W_2 * h_{t-1})\)</span>.</p></li><li><p>The controller predicts <em>ElemMult</em> and <em>ReLU</em> for tree index 1, this means we need to compute <span class="math inline">\(a_1 = \text{ReLU} ((W_3 * x_t) \odot (W_4 * h_{t-1}))\)</span>.</p></li><li><p>The controller predicts 0 for the second element of the “Cell Index”, <em>Add</em> and <em>ReLU</em> for elements in “Cell Inject”, which means we need to compute <span class="math inline">\(a^{new}_0 = \text{ReLU}(a_0 + c_{t-1})\)</span>. Notice that we don’t have any learnable parameters for the internal nodes of the tree.</p></li><li><p>The controller predicts <em>ElemMult</em> and <em>Sigmoid</em> for tree index 2, this means we need to compute <span class="math inline">\(a_2 = \text{sigmoid}(a^{new}_0 \odot a_1)\)</span>. Since the maximum index in the tree is 2, <span class="math inline">\(h_t\)</span> is set to <span class="math inline">\(a_2\)</span>.</p></li><li><p>The controller RNN predicts 1 for the first element of the “Cell Index”, this means that we should set <span class="math inline">\(c_t\)</span> to the output of the tree at index 1 before the activation, i.e., <span class="math inline">\(c_t = (W_3 * x_t) \odot (W_4 * h_{t-1})\)</span>.</p></li></ol><h2 id="experiments-and-results">Experiments and Results</h2><p>On CIFAR-10, our goal is to find a good convolutional architecture whereas on Penn Treebank our goal is to find a good recurrent cell.</p><h3 id="learning-convolutional-architectures-for-cifar-10">Learning Convolutional Architectures for CIFAR-10</h3><p><strong>Search space</strong>: Our search space consists of convolutional architectures, with rectified linear units as non-linearities (Nair &amp; Hinton, 2010), batch normalization (Ioffe &amp; Szegedy, 2015) and skip connections between layers (Section 3.3). For every convolutional layer, the controller RNN has to select a filter height in [1, 3, 5, 7], a filter width in [1, 3, 5, 7], and a number of filters in [24, 36, 48, 64]. For strides, we perform two sets of experiments, one where we fix the strides to be 1, and one where we allow the controller to predict the strides in [1, 2, 3].</p><p><strong>Training details</strong>: The controller RNN is a two-layer LSTM with 35 hidden units on each layer. It is trained with the ADAM optimizer (Kingma &amp; Ba, 2015) with a learning rate of 0.0006. The weights of the controller are initialized uniformly between -0.08 and 0.08. For the distributed training, we set the number of parameter server shards S to 20, the number of controller replicas K to 100 and the number of child replicas m to 8, which means there are 800 networks being trained on 800 GPUs concurrently at any time.</p><p>Once the controller RNN samples an architecture, a child model is constructed and trained for 50 epochs. The reward used for updating the controller is the maximum validation accuracy of the last 5 epochs cubed. The validation set has 5,000 examples randomly sampled from the training set, the remaining 45,000 examples are used for training. The settings for training the CIFAR-10 child models are the same with those used in Huang et al. (2016a). We use the Momentum Optimizer with a learning rate of 0.1, weight decay of 1e-4, momentum of 0.9 and used Nesterov Momentum (Sutskever et al., 2013).</p><p>During the training of the controller, we use a schedule of increasing number of layers in the child networks as training progresses. On CIFAR-10, we ask the controller to increase the depth by 2 for the child models every 1,600 samples, starting at 6 layers.</p><p><strong>Results</strong>: After the controller trains 12,800 architectures, we find the architecture that achieves the best validation accuracy. We then run a small grid search over learning rate, weight decay, batchnorm epsilon and what epoch to decay the learning rate.</p><p><img src="table1.png"></p><p>Table 1: Performance of Neural Architecture Search and other state-of-the-art models on CIFAR-10.</p><h3 id="learning-recurrent-cells-for-penn-treebank">Learning Recurrent Cells for Penn Treebank</h3><p><strong>Search space</strong>: For each node in the tree, the controller RNN needs to select a combination method in [add; elem mult] and an activation method in [identity; tanh; sigmoid; relu]. The number of input pairs to the RNN cell is called the “base number” and set to 8 in our experiments. When the base number is 8, the search space is has approximately <span class="math inline">\(6 \times 10^{16}\)</span> architectures, which is much larger than 15,000, the number of architectures that we allow our controller to evaluate.</p><p><strong>Training details</strong>: The controller and its training are almost identical to the CIFAR-10 experiments except for a few modifications: 1) the learning rate for the controller RNN is 0.0005, slightly smaller than that of the controller RNN in CIFAR-10, 2) in the distributed training, we set S to 20, K to 400 and m to 1, which means there are 400 networks being trained on 400 CPUs concurrently at any time, 3) during asynchronous training we only do parameter updates to the parameter-server once 10 gradients from replicas have been accumulated.</p><p>In our experiments, every child model is constructed and trained for 35 epochs.</p><p><img src="table2.png"></p><p>Table 2: Single model perplexity on the test set of the Penn Treebank language modeling task. Parameter numbers with z are estimates with reference to Merity et al. (2016).</p><h2 id="conclusion">Conclusion</h2><p>In this paper we introduce Neural Architecture Search, an idea of using a recurrent neural network to compose neural network architectures.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(ICLR 2017) Neural Architecture Search with Reinforcement Learning&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1611.01578&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1611.01578&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Page: &lt;a href=&quot;https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
      <category term="Paper" scheme="http://lijiancheng0614.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>NASNet</title>
    <link href="http://lijiancheng0614.github.io/2018/10/29/2018_10_29_NASNet/"/>
    <id>http://lijiancheng0614.github.io/2018/10/29/2018_10_29_NASNet/</id>
    <published>2018-10-28T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.812Z</updated>
    
    <content type="html"><![CDATA[<p>(CVPR 2018) Learning Transferable Architectures for Scalable Image Recognition</p><p>Paper: <a href="https://arxiv.org/abs/1707.07012" class="uri" target="_blank" rel="noopener">https://arxiv.org/abs/1707.07012</a></p><p>Page: <a href="https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html" class="uri" target="_blank" rel="noopener">https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html</a></p><p>Code: <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet" class="uri" target="_blank" rel="noopener">https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet</a></p><a id="more"></a><h2 id="introduction">Introduction</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(CVPR 2018) Learning Transferable Architectures for Scalable Image Recognition&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1707.07012&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1707.07012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Page: &lt;a href=&quot;https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="Paper" scheme="http://lijiancheng0614.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>使用 TensorFlow Attention OCR API 进行图像文本提取</title>
    <link href="http://lijiancheng0614.github.io/2018/10/02/2018_10_02_TensorFlow-Attention-OCR/"/>
    <id>http://lijiancheng0614.github.io/2018/10/02/2018_10_02_TensorFlow-Attention-OCR/</id>
    <published>2018-10-01T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.809Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/attention_ocr" class="uri" target="_blank" rel="noopener">https://github.com/tensorflow/models/tree/master/research/attention_ocr</a></p><p>使用 TensorFlow Attention OCR API 进行图像文本提取</p><p>修改后的代码：<a href="https://github.com/lijiancheng0614/tensorflow_attention_ocr" class="uri" target="_blank" rel="noopener">https://github.com/lijiancheng0614/tensorflow_attention_ocr</a></p><a id="more"></a><p><img src="https://raw.githubusercontent.com/tensorflow/models/master/research/attention_ocr/python/testdata/fsns_train_00.png"></p><h2 id="准备">准备</h2><ol start="0" type="1"><li><p>文件结构</p><p>为了方便查看文件，使用以下文件结构。</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tensorflow_attention_ocr</span><br><span class="line">+-- README.md</span><br><span class="line">+-- common_flags.py</span><br><span class="line">+-- data_provider.py</span><br><span class="line">+-- datasets</span><br><span class="line">|   +-- __init__.py</span><br><span class="line">|   +-- data</span><br><span class="line">|   |   +-- fsns</span><br><span class="line">|   |       +-- charset_size=134.txt</span><br><span class="line">|   |       +-- fsns-00000-of-00001</span><br><span class="line">|   |       +-- links.txt</span><br><span class="line">|   |       +-- testdata</span><br><span class="line">|   |           +-- fsns_train_00.png</span><br><span class="line">|   |           +-- ...</span><br><span class="line">|   |           +-- fsns_train_31.png</span><br><span class="line">|   +-- fsns.py</span><br><span class="line">+-- eval.py</span><br><span class="line">+-- inception_preprocessing.py</span><br><span class="line">+-- infer.py</span><br><span class="line">+-- metrics.py</span><br><span class="line">+-- model.py</span><br><span class="line">+-- sequence_layers.py</span><br><span class="line">+-- train.py</span><br><span class="line">+-- utils.py</span><br></pre></td></tr></table></figure></p></li><li><p>安装 TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="noopener">https://www.tensorflow.org/install/</a></p></li><li><p>准备数据</p><p>需要数据集的 <code>charset</code> 和 TFRecord，这里使用官方生成的 fsns 数据集的 TFRecord。</p><p>生成其它数据集的 TFRecord 见 <a href="https://github.com/lijiancheng0614/tensorflow_attention_ocr/tree/develop" class="uri" target="_blank" rel="noopener">https://github.com/lijiancheng0614/tensorflow_attention_ocr/tree/develop</a></p></li><li><p>（可选）下载模型</p><p>官方提供了一个训练好的模型，可用于 finetune 和测试。</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.tensorflow.org/models/attention_ocr_2017_08_09.tar.gz</span><br><span class="line">tar zxf attention_ocr_2017_08_09.tar.gz</span><br></pre></td></tr></table></figure></p></li></ol><h2 id="训练">训练</h2><p>如果使用现有模型进行预测则不需要训练。</p><ol type="1"><li><p>训练</p><p>从头开始训练：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python train.py \</span><br><span class="line">    --train_log_dir=train_logs</span><br></pre></td></tr></table></figure></p><p>使用预训练的 Inception 权重初始化训练模型：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python train.py \</span><br><span class="line">    --checkpoint_inception=inception_v3.ckpt</span><br></pre></td></tr></table></figure></p><p>使用一个 checkpoint 进行 fine tune：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python train.py \</span><br><span class="line">    --checkpoint=model.ckpt-399731</span><br></pre></td></tr></table></figure></p></li><li><p>验证</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python eval.py \</span><br><span class="line">    --train_log_dir=train_logs \</span><br><span class="line">    --eval_log_dir=eval_logs \</span><br><span class="line">    --num_batches=1</span><br></pre></td></tr></table></figure></p></li><li><p>可视化 log</p><p>可一边训练一边可视化训练的 log，访问 <code>http://localhost:8001/</code> 即可看到 loss 等的变化。</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir . --port 8001</span><br></pre></td></tr></table></figure></p></li></ol><h2 id="测试">测试</h2><p>测试图片为 <code>datasets/data/fsns/testdata/fsns_train_%02d.png</code>，运行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python infer.py \</span><br><span class="line">    --checkpoint=model.ckpt-399731 \</span><br><span class="line">    --image_path_pattern=datasets/data/fsns/testdata/fsns_train_%02d.png</span><br></pre></td></tr></table></figure><p>控制台中将输出每张图片的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Boulevard de Lunel░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue de Provence░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue de Port Maria░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Avenue Charles Gounod░░░░░░░░░░░░░░░░</span><br><span class="line">Rue de l‘Aurore░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue de Beuzeville░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue d‘Orbey░░░░░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue Victor Schoulcher░░░░░░░░░░░░░░░░</span><br><span class="line">Rue de la Gare░░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue des Tulipes░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue André Maginot░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Route de Pringy░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue des Landelles░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue des Ilettes░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Avenue de Maurin░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue Théresa░░░░░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Route de la Balme░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue Hélène Roederer░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue Emile Bernard░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Place de la Mairie░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue des Perrots░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue de la Libération░░░░░░░░░░░░░░░░░</span><br><span class="line">Impasse du Capcir░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Avenue de la Grand Mare░░░░░░░░░░░░░░</span><br><span class="line">Rue Pierre Brossolette░░░░░░░░░░░░░░░</span><br><span class="line">Rue de Provence░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue du Docteur Mourre░░░░░░░░░░░░░░░░</span><br><span class="line">Rue d‘Ortheuil░░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue des Sarments░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Rue du Centre░░░░░░░░░░░░░░░░░░░░░░░░</span><br><span class="line">Impasse Pierre Mourgues░░░░░░░░░░░░░░</span><br><span class="line">Rue Marcel Dassault░░░░░░░░░░░░░░░░░░</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/attention_ocr&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tensorflow/models/tree/master/research/attention_ocr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用 TensorFlow Attention OCR API 进行图像文本提取&lt;/p&gt;
&lt;p&gt;修改后的代码：&lt;a href=&quot;https://github.com/lijiancheng0614/tensorflow_attention_ocr&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/lijiancheng0614/tensorflow_attention_ocr&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Week 6 Bayesian Games</title>
    <link href="http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_6/"/>
    <id>http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_6/</id>
    <published>2018-06-14T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.809Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 6: Bayesian Games</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/6" class="uri" target="_blank" rel="noopener">https://www.coursera.org/learn/game-theory-1/home/week/6</a></p><a id="more"></a><h2 id="bayesian-games-taste">6-1 Bayesian Games: Taste</h2><p>例子：拍卖</p><h2 id="bayesian-games-first-definition">6-2 Bayesian Games: First Definition</h2><ul><li><p>Bayesian Game: Information Sets</p><p>贝叶斯博弈</p><p>A <strong>Bayesian game</strong> is a tuple <span class="math inline">\((N, G, P, I)\)</span> where</p><ul><li><p><span class="math inline">\(N\)</span> is a set of agents</p></li><li><p><span class="math inline">\(G\)</span> is a set of games with <span class="math inline">\(N\)</span> agents each such that if <span class="math inline">\(g, g&#39; \in G\)</span> then for each agent <span class="math inline">\(i \in N\)</span> the strategy space in <span class="math inline">\(g\)</span> is identical to the strategy space in <span class="math inline">\(g&#39;\)</span></p></li><li><p><span class="math inline">\(P \in \prod (G)\)</span> is a common prior over games, where <span class="math inline">\(\prod(G)\)</span> is the set of all probability distributions over <span class="math inline">\(G\)</span></p></li><li><p><span class="math inline">\(I = (I_1, \cdots, I_N)\)</span> is a set of partitions of <span class="math inline">\(G\)</span>, one for each agent</p></li></ul></li></ul><h2 id="bayesian-games-first-defintion-yoav">6-2 Bayesian Games: First Defintion (yoav)</h2><h2 id="bayesian-games-second-definition">6-3 Bayesian Games: Second Definition</h2><ul><li><p>Directly represent uncertainty over utility function using the notion of <strong>epistemic type</strong>.</p></li><li><p>Bayesian Game: Epistemic Types</p><p>A <strong>Bayesian game</strong> is a tuple <span class="math inline">\((N, A, \Theta, p, u)\)</span> where</p><ul><li><p><span class="math inline">\(N\)</span> is a set of agents</p></li><li><p><span class="math inline">\(A = (A_1, \cdots, A_n)\)</span>, where <span class="math inline">\(A_i\)</span> is the set of actions available to player <span class="math inline">\(i\)</span></p></li><li><p><span class="math inline">\(\Theta = (\Theta_1, \cdots, \Theta_n)\)</span>, where <span class="math inline">\(\Theta_i\)</span> is the type space of player <span class="math inline">\(i\)</span></p></li><li><p><span class="math inline">\(p : \Theta \mapsto [0, 1]\)</span> is the common prior over types</p></li><li><p><span class="math inline">\(u = (u_1, \cdots, u_n)\)</span>, where <span class="math inline">\(u_i : A \times \Theta \mapsto \mathbb{R}\)</span> is the utility function for player <span class="math inline">\(i\)</span></p></li></ul></li></ul><h2 id="analyzing-bayesian-games">6-4 Analyzing Bayesian Games</h2><ul><li><p>Expected Utility</p><p>期望效用</p><p>3 standard notions:</p><ol type="1"><li><p>ex-ante</p><p>the agent know nothing about anyone's actual type</p><p>事前：该代理不知道其它代理的实际类型</p></li><li><p>interim</p><p>an agent knows her own type but not the types of the other agents</p><p>中期：该代理知道自己的类型，但不知道其它代理的类型</p></li><li><p>ex-post</p><p>the agent knows all agents' types</p><p>事后：该代理知道所有代理的类型</p></li></ol></li></ul><p>Given a Bayesian game <span class="math inline">\((N, A, \Theta, p, u)\)</span>, where <span class="math inline">\(i\)</span>’s type is <span class="math inline">\(\theta_i\)</span> and where the agents’ strategies are given by the mixed strategy profile <span class="math inline">\(s\)</span></p><ul><li><p>Interim expected utility</p><p><span class="math display">\[EU_i(s \mid \theta_i) = \sum_{\theta_{-i} \in \Theta_{-i}} p(\theta_{-i} \mid \theta_i) \sum_{a \in A}  \left( \prod_{j \in N} s_j(a_j \mid \theta_j) \right) u_i(a, \theta_{-i}, \theta_i)\]</span></p></li><li><p>Ex-ante expected utility</p><p><span class="math display">\[EU_i(s) = \sum_{\theta_{-i} \in \Theta_{-i}} p(\theta_{-i}) EU_i(s \mid \theta_i)\]</span></p></li><li><p>Ex-post expected utility</p><p><span class="math display">\[EU_i(s, \theta) = \sum_{a \in A} \left( \prod_{j \in N} s_j(a_j \mid \theta_j) \right) u_i(a, \theta)\]</span></p></li><li><p>Best response</p><p><span class="math display">\[BR_i(s_{-i}) = \text{argmax}_{s&#39;_i} EU_i(s&#39;_i, s_{-i} \mid \theta_i)\]</span></p><p>if <span class="math inline">\(\forall \theta_i \in \Theta_i, p(\theta_i) \gt 0\)</span></p><p><span class="math display">\[BR_i(s_{-i}) = \text{argmax}_{s&#39;_i} EU_i(s&#39;_i, s_{-i}) = \text{argmax}_{s&#39;_i} \sum_{\theta_i} EU_i(s&#39;_i, s_{-i} \mid \theta_i)\]</span></p></li><li><p>Bayesian equilibrium / Bayes-Nash equilibrium</p><p><span class="math display">\[\forall i, s_i \in BR_i(s_{-i})\]</span></p></li></ul><h2 id="analyzing-bayesian-games-another-example">6-5 Analyzing Bayesian Games: Another Example</h2><p>例子：A Sheriff's Dilemma</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 6: Bayesian Games&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/6&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/6&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Week 7 Coalitional Games</title>
    <link href="http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_7/"/>
    <id>http://lijiancheng0614.github.io/2018/06/15/2018_06_15_Game_Theory_Week_7/</id>
    <published>2018-06-14T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.809Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 7: Coalitional Games</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/7" class="uri" target="_blank" rel="noopener">https://www.coursera.org/learn/game-theory-1/home/week/7</a></p><a id="more"></a><h2 id="coalitional-game-theory-taste">7-1 Coalitional Game Theory: Taste</h2><p>举例：政治伙伴、商业合作联盟、建筑团队</p><h2 id="coalitional-game-theory-definitions">7-2 Coalitional Game Theory: Definitions</h2><ul><li><p>Transferable utility assumption:</p><p>可传递效用假设：</p><ol type="1"><li><p>the payoffs to a coalition may be freely redistributed among its members</p><p>联盟的收益可以在其成员之间自由重新分配</p></li><li><p>satisfied whenever there is a universal currency that is used for exchange in the system</p><p>满足只要有用于系统交换的通用货币</p></li><li><p>means that each coalition can be assigned a single value as its payoff</p><p>意味着每个联盟可以分配一个单一的价值作为其收益</p></li></ol></li><li><p>Coalitional game with transferable utility</p><p>带可传递效用的联盟型博弈（合作博弈 Cooperative game）</p><p>a pair <span class="math inline">\((N, v)\)</span>, where</p><ul><li><p><span class="math inline">\(N\)</span> is a finite set of players, indexed by i</p></li><li><p><span class="math inline">\(v : 2^N \mapsto \mathbb{R}\)</span> associates with each coalition <span class="math inline">\(S \subseteq N\)</span> a real-valued payoff <span class="math inline">\(v(S)\)</span> that the coalition’s members can distribute among themselves. We assume that <span class="math inline">\(v(\emptyset) = 0\)</span></p></li></ul></li><li><p>Superadditive game</p><p>超可加的博弈</p><p>A game <span class="math inline">\(G = (N, v)\)</span> is <strong>superadditive</strong> if for all <span class="math inline">\(S, T \subset N\)</span>, if <span class="math inline">\(S \cap T = \emptyset\)</span>, then <span class="math inline">\(v(S \cup T) \ge v(S) + v(T)\)</span></p></li></ul><h2 id="the-shapley-value">7-3 The Shapley Value</h2><p><span class="math inline">\(\psi(N, v)\)</span> is a vector of payoffs to each agent, explaining how they divide the payoff of the grand coalition, <span class="math inline">\(\psi_i(N, v)\)</span> is <span class="math inline">\(i\)</span>'s payoff.</p><ol type="1"><li><p>Axiom (Symmetry)</p><p>For any <span class="math inline">\(v\)</span>, if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are interchangeable then <span class="math inline">\(\psi_i(N, v) = \psi_j(N, v)\)</span></p></li><li><p>Axiom (Dummy player)</p><p>For any <span class="math inline">\(v\)</span>, if <span class="math inline">\(i\)</span> is a dummy player then <span class="math inline">\(\psi_i(N, v) = v({i})\)</span></p></li><li><p>Axiom (Additivity)</p><p>For any two <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>, <span class="math inline">\(\psi_i(N, v_1 + v_2) = \psi_i(N, v_1) + \psi_i(N, v_2)\)</span> for each <span class="math inline">\(i\)</span>, where the game <span class="math inline">\((N, v_1 + v_2)\)</span> is defined by <span class="math inline">\((v_1 + v_2)(S) = v_1(S) + v_2(S)\)</span> for every coalition <span class="math inline">\(S\)</span></p></li></ol><ul><li><p>Shapley value</p><p>Given a coalitional game <span class="math inline">\((N, v)\)</span>, the <strong>Shapley value</strong> divides payoffs among players according to:</p><p><span class="math display">\[\phi_i(N, v) = \frac{1}{N!} \sum_{S \subseteq N \backslash \{i\}} \lvert S \rvert ! (\lvert N \rvert - \lvert S \rvert - 1)! \left[ v(S \cup \{i\}) - v(S) \right]\]</span></p></li><li><p>Theorem</p><p>Given a coalitional game <span class="math inline">\((N, v)\)</span>, there is a unique payoff division <span class="math inline">\(x(v) = \phi(N, v)\)</span> that divides the full payoff of the grand coalition and that satisfies the Symmetry, Dummy player and Additivity axioms: the Shapley Value</p></li></ul><h2 id="the-core">7-4 The Core</h2><ul><li><p>Core</p><p>核心</p><p>A payoff vector <span class="math inline">\(x\)</span> is in the <strong>core</strong> of a coalitional game <span class="math inline">\((N, v)\)</span> if and only if</p><p><span class="math display">\[\forall S \subseteq N, \sum_{i \in S} x_i \ge v(S)\]</span></p></li></ul><p>The core can be empty, and not unique.</p><p>核心可以为空，也可以不唯一。</p><ul><li><p>Simple game</p><p>简单博弈</p><p>A game <span class="math inline">\(G = (N, v)\)</span> is <strong>simple</strong> if for all <span class="math inline">\(S \subset N\)</span>, <span class="math inline">\(v(S) \in {0, 1}\)</span></p></li><li><p>Veto player</p><p>否决选手</p><p>A player <span class="math inline">\(i\)</span> is a <strong>veto player</strong> if <span class="math inline">\(v(N \backslash \{i\}) = 0\)</span></p></li><li><p>Theorem</p><p>In a simple game the core is empty iff there is no veto player.</p><p>在一个简单博弈中，当且仅当没有否决选手，核心为空。</p><p>If there are veto players, the core consists of all payoff vectors in which the nonveto players get 0.</p><p>如果有否决选手，则核心由所有非否决权者收益为 0 的分配向量组成。</p></li></ul><p>Example: Airport Game</p><ul><li><p>Convex game</p><p>凸博弈</p><p>A game <span class="math inline">\(G = (N, v)\)</span> is <strong>convex</strong> if for all <span class="math inline">\(S, T \subset N\)</span>, <span class="math inline">\(v(S \cup T) \ge v(S) + v(T) - v(S \cap T)\)</span></p></li><li><p>Theorem</p><p>Every convex game has a nonempty core.</p><p>每个凸博弈都有非空核心。</p></li><li><p>Theorem</p><p>In every convex game, the Shapley value is in the core.</p><p>对于每个凸博弈，Shapley 值都在核心中。</p></li></ul><h2 id="comparing-the-core-and-shapley-value-in-an-example">7-5 Comparing the Core and Shapley value in an Example</h2><p>UN security council: represent it as a cooperative game</p><p>举例：联合国安理会（可以看作合作博弈）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 7: Coalitional Games&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/7&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/7&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Week 2 Mixed-Strategy Nash Equilibrium</title>
    <link href="http://lijiancheng0614.github.io/2018/06/14/2018_06_14_Game_Theory_Week_2/"/>
    <id>http://lijiancheng0614.github.io/2018/06/14/2018_06_14_Game_Theory_Week_2/</id>
    <published>2018-06-13T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.809Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 2: Mixed-Strategy Nash Equilibrium</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/2" class="uri" target="_blank" rel="noopener">https://www.coursera.org/learn/game-theory-1/home/week/2</a></p><a id="more"></a><h2 id="mixed-strategies-and-nash-equilibrium-i">2-1 Mixed Strategies and Nash Equilibrium (I)</h2><p>举例：设置检查站，攻击与防守</p><h2 id="mixed-strategies-and-nash-equilibrium-ii">2-2 Mixed Strategies and Nash Equilibrium (II)</h2><p>Define a <strong>strategy</strong> <span class="math inline">\(s_i\)</span> for agent <span class="math inline">\(i\)</span> as any probability distribution over the actions <span class="math inline">\(A_i\)</span>.</p><ul><li><p><strong>pure strategy</strong>: only one action is played with positive probability</p></li><li><p><strong>mixed strategy</strong>: more than one action is played with positive probability</p><p>these actions are called the <strong>support</strong> of the mixed strategy</p></li></ul><p><span class="math inline">\(S_i\)</span>: all strategies for <span class="math inline">\(i\)</span></p><p><span class="math inline">\(S = S_1 \times \cdots \times S_n\)</span>: all strategy profiles</p><ul><li>expected utility</li></ul><p><span class="math display">\[u_i(s) = \sum_{a \in A} u_i(a) Pr(a \mid s)\]</span></p><p><span class="math display">\[Pr(a \mid s) = \prod_{j \in N} s_j(a_j)\]</span></p><ul><li><p>Theorem (Nash, 1950) 定理（纳什，1950）</p><p>Every finite game has a Nash equilibrium.</p><p>每一个有限博弈都有纳什均衡。</p></li></ul><h2 id="computing-mixed-nash-equilibrium">2-3 Computing Mixed Nash Equilibrium</h2><p>概率计算</p><h2 id="hardness-beyond-2x2-games---basic">2-4 Hardness Beyond 2x2 Games - Basic</h2><p>2 example algorithms to finding Nash Equilibrium:</p><p>2 个寻找纳什均衡的算法：</p><ul><li><p>LCP (Linear Complementarity) formulation</p></li><li><p>Support Enumeration Method</p></li></ul><p>PPAD (Polynomial Parity Arguments on Directed graphs)</p><ul><li><p>Theorem</p><p>Computing a Nash equilibrium is PPAD-complete</p></li></ul><h2 id="hardness-beyond-2x2-games---advanced">2-4 Hardness Beyond 2x2 Games - Advanced</h2><ul><li><p>LCP (Linear Complementarity) formulation</p><ol type="1"><li><p>Finding a NE with a specific support</p></li><li><p>Smart heuristic search through all sets of support</p></li></ol></li></ul><p>the following are NP-complete:</p><ol type="1"><li><p>(Uniqueness) Given a game <span class="math inline">\(G\)</span>, does there exist a unique equilibrium in <span class="math inline">\(G\)</span>?</p></li><li><p>(Pareto optimality) Given a game <span class="math inline">\(G\)</span>, does there exist a strictly Pareto efficient equilibrium in <span class="math inline">\(G\)</span>?</p></li><li><p>(Guaranteed payoff) Given a game <span class="math inline">\(G\)</span> and a value <span class="math inline">\(v\)</span>, does there exist an equilibrium in <span class="math inline">\(G\)</span> in which some player <span class="math inline">\(i\)</span> obtains an expected payoff of at least <span class="math inline">\(v\)</span>?</p></li><li><p>(Guaranteed social welfare) Given a game <span class="math inline">\(G\)</span>, does there exist an equilibrium in which the sum of agents' utilities is at least <span class="math inline">\(k\)</span>?</p></li><li><p>(Action inclusion) Given a game <span class="math inline">\(G\)</span> and an action <span class="math inline">\(a_i \in A_i\)</span> for some player <span class="math inline">\(i \in N\)</span>, does there exist an equilibrium of <span class="math inline">\(G\)</span> in which player <span class="math inline">\(i\)</span> plays action <span class="math inline">\(a_i\)</span> with strictly positive probability?</p></li><li><p>(Action exclusion) Given a game <span class="math inline">\(G\)</span> and an action <span class="math inline">\(a_i \in A_i\)</span> for some player <span class="math inline">\(i \in N\)</span>, does there exist an equilibrium of <span class="math inline">\(G\)</span> in which player <span class="math inline">\(i\)</span> plays action <span class="math inline">\(a_i\)</span> with zero probability?</p></li></ol><h2 id="example-mixed-strategy-nash">2-5 Example: Mixed Strategy Nash</h2><p>Example: Soccer Penalty Kicks</p><p>举例：足球罚球</p><h2 id="data-professional-sports-and-mixed-strategies">2-6 Data: Professional Sports and Mixed Strategies</h2><p>上节例子的数据分析</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 2: Mixed-Strategy Nash Equilibrium&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/2&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/2&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Game Theory Week 1 Introduction and Overview</title>
    <link href="http://lijiancheng0614.github.io/2018/06/11/2018_06_11_Game_Theory_Week_1/"/>
    <id>http://lijiancheng0614.github.io/2018/06/11/2018_06_11_Game_Theory_Week_1/</id>
    <published>2018-06-10T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.809Z</updated>
    
    <content type="html"><![CDATA[<p>Game Theory</p><p>Week 1: Introduction and Overview</p><p><a href="https://www.coursera.org/learn/game-theory-1/home/week/1" class="uri" target="_blank" rel="noopener">https://www.coursera.org/learn/game-theory-1/home/week/1</a></p><a id="more"></a><h2 id="game-theory-intro---tcp-backoff">1-1 Game Theory Intro - TCP Backoff</h2><p>从 TCP 协议中的退避机制（backoff mechanisn）引出博弈：</p><p>A game in general is any interaction between two or more people where the outcomes of the interaction depend on what everybody does and everybody has different levels of happiness for the different outcomes.</p><p>博弈就是两或多人间的互动，其中互动的结果取决于每个人的行为，并且每个人对于不同的结果都会有不同的愉悦度。</p><h2 id="self-interested-agents-and-utility-theory">1-2 Self-Interested Agents and Utility Theory</h2><p>utility function is, a mathematical measure that tells you how much the agent likes or does not like a given situation.</p><p>收益函数是指：一个数学的评估方法，用于决定一个代理对于特定情况的喜恶程度。</p><p>And the decision theoretic approach which is what underlies modern game theory, says that you're going to try to act in the way that maximizes your expected or average utility.</p><p>对于这种喜好的理论决定方法，就是现代博弈论的基础。这个基础就是，每个人都试图将期望效益最大化。</p><h2 id="defining-games">1-3 Defining Games</h2><ul><li><p>Normal Form (a.k.a. Matrix Form, Strategic Form)</p><p>Finite, n-person normal form game: <span class="math inline">\(&lt;N, A, u&gt;\)</span></p><ul><li><p>Players</p><p><span class="math inline">\(N = \{1, \cdots, n\}\)</span></p></li><li><p>Actions</p><p>Action set for player <span class="math inline">\(i\)</span>: <span class="math inline">\(A_i\)</span></p></li><li><p>Payoffs</p><p>Utility function or Payoff function for player <span class="math inline">\(i\)</span>: <span class="math inline">\(u_i: A \mapsto \mathbb{R}\)</span></p></li></ul></li><li><p>Extensive Form</p><ul><li><p>Timing</p></li><li><p>Information</p></li></ul></li></ul><h2 id="examples-of-games">1-4 Examples of Games</h2><ul><li><p>Games of Cooperation</p></li><li><p>Coordination Game</p></li><li><p>General Games</p></li></ul><h2 id="nash-equilibrium-intro">1-5 Nash Equilibrium Intro</h2><ul><li><p>Keynes Beauty Contest Game</p></li><li><p>Keynes Beauty Contest Game: The Stylized Version</p></li></ul><h2 id="strategic-reasoning">1-6 Strategic Reasoning</h2><p>Nash Equilibrium:</p><p>纳什均衡：</p><ul><li><p>A consistent list of actions</p></li><li><p>Each player's action maximizes his or her payoff given the actions of the others</p><p>给定其他人的行为，每个玩家的行为都会最大化其收益</p></li><li><p>A self-consistent or stable profile</p></li></ul><h2 id="best-response-and-nash-equilibrium">1-7 Best Response and Nash Equilibrium</h2><p>Let <span class="math inline">\(a_{-i} = &lt;a_1, \cdots, a_{i - 1}, a_{i + 1}, \cdots, a_n&gt;\)</span></p><p>then <span class="math inline">\(a = (a_{-i}, a_i)\)</span></p><ul><li><p>Best Response</p><p><span class="math inline">\(a_i^* \in BR(a_{-i}) \iff \forall a_i \in A_i, u_i(a_i^*, a_{-i}) \ge u_i(a_i, a_{-i})\)</span></p></li><li><p>Nash Equilibrium</p><p><span class="math inline">\(a = &lt;a_1, \cdots, a_n&gt;\)</span> is a ("pure strategy") Nash equilibrium iff <span class="math inline">\(\forall i, a_i \in BR(a_{-i})\)</span></p></li></ul><h2 id="nash-equilibrium-of-example-games">1-8 Nash Equilibrium of Example Games</h2><h2 id="dominant-strategies">1-9 Dominant Strategies</h2><p>2 strategies <span class="math inline">\(s_i\)</span>, <span class="math inline">\(s&#39;_i\)</span></p><p>the set of all possible strategy profiles for the other players: <span class="math inline">\(S_{-i}\)</span></p><ul><li><p><span class="math inline">\(s_i\)</span> strictly dominates <span class="math inline">\(s&#39;_i\)</span></p><p>if <span class="math inline">\(\forall s_{-i} \in S_{-i}, u_i(s_i, s_{-i}) \gt u_i(s&#39;_i, s_{-i})\)</span></p></li><li><p><span class="math inline">\(s_i\)</span> very weakly dominates <span class="math inline">\(s&#39;_i\)</span></p><p>if <span class="math inline">\(\forall s_{-i} \in S_{-i}, u_i(s_i, s_{-i}) \ge u_i(s&#39;_i, s_{-i})\)</span></p></li><li><p>dominant 主导</p><p>If one strategy dominates all others.</p></li><li><p>A strategy profile consisting of dominant strategies for every player must be a Nash equilibrium.</p></li></ul><h2 id="pareto-optimality">1-10 Pareto Optimality</h2><ul><li><p><span class="math inline">\(o\)</span> Pareto-dominate <span class="math inline">\(o&#39;\)</span></p><p>one outcome <span class="math inline">\(o\)</span> is at least as good for every agent as another outcome <span class="math inline">\(o&#39;\)</span>.</p></li><li><p><span class="math inline">\(o^*\)</span> is Pareto Optimality 帕累托最优</p><p>if there is no other outcome that Pareto-dominates it.</p></li></ul><p>Every game has at least 1 Pareto-optimal outcome, and can have more than one Pareto-optimal outcome.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Game Theory&lt;/p&gt;
&lt;p&gt;Week 1: Introduction and Overview&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/game-theory-1/home/week/1&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/game-theory-1/home/week/1&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study" scheme="http://lijiancheng0614.github.io/categories/Study/"/>
    
    
      <category term="Note" scheme="http://lijiancheng0614.github.io/tags/Note/"/>
    
      <category term="Coursera" scheme="http://lijiancheng0614.github.io/tags/Coursera/"/>
    
      <category term="Game Theory" scheme="http://lijiancheng0614.github.io/tags/Game-Theory/"/>
    
  </entry>
  
  <entry>
    <title>文本自动摘要</title>
    <link href="http://lijiancheng0614.github.io/2018/05/12/2018_05_12_text_summarization/"/>
    <id>http://lijiancheng0614.github.io/2018/05/12/2018_05_12_text_summarization/</id>
    <published>2018-05-11T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.809Z</updated>
    
    <content type="html"><![CDATA[<p>文本自动摘要（自动文摘）Text Summarization 指自动地从原始文档中提取摘要，摘要是全面准确地反映该文档中心内容的简单连贯的短文。</p><a id="more"></a><h2 id="应用">应用</h2><p>学术文献、 会议记录、 电影剧本、学生反馈、软件代码、 直播文字</p><h2 id="评价指标">评价指标</h2><h3 id="人工评价">人工评价</h3><p>时间成本太高，效率太低</p><h3 id="自动评价">自动评价</h3><p>给定参考摘要作为标准答案，通过制定一些规则来给生产的摘要打分。</p><p>ROUGE 系统（Recall-Oriented Understudy for Gisting Evaluation）：将待审的摘要和参考摘要的 n 元组共现统计量作为评价依据，通过一系列标准进行打分。</p><p>包括 ROUGE-N (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4), ROUGE-L, ROUGE-W，ROUGE-S, ROUGE-SU</p><h2 id="方法">方法</h2><h3 id="抽取式摘要-extraction-based-summarization">抽取式摘要 Extraction-based summarization</h3><p>从原文中找到一些关键的句子，组合成一篇摘要。</p><ol type="1"><li><p>基于特征</p><p>统计句子包含的关键词数量、关键词位置、句子长度、句子位置等。</p><p>方法：TextTeaser</p><p>论文：</p><ul><li><p>(IBM Journal 1958) The Automatic Creation of Literature Abstracts</p></li><li><p>(Journal of the ACM 1969) New Methods in Automatic Extracting</p></li><li><p>(SIGIR 2001) Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis</p></li></ul></li><li><p>基于图排序</p><p>将文档的每句话作为节点，句子之间的相似度作为边权，构建图模型，计算每个句子的得分。</p><p>方法：LexRank, TextRank</p><p>论文：</p><ul><li><p>(JAIR 2004) LexRank: Graph-based Lexical Centrality as Salience in Text Summarization</p></li><li><p>(EMNLP 2004) TextRank: Bringing Order into Texts</p></li></ul></li><li><p>神经网络</p><p>方法：Attention Model, RNN, CNN</p><p>论文：</p><ul><li>(ACL 2016) Neural Summarization by Extracting Sentences and Words</li></ul></li></ol><h3 id="综合式摘要-abstractive-summarization">综合式摘要 Abstractive Summarization</h3><p>理解原文并用简洁文本表达。</p><p>方法：</p><ul><li><p>Encoder-Decoder 框架</p><p>Encoder 是将输入序列表示成一个带有语义的向量，通常使用 LSTM、GRU 等 RNN 模型，复杂的也有 BiRNN、BiRNN with LSTM、BiRNN with GRU、多层RNN等模型。</p><p>Decoder 是以 Encoder 输出的向量作为输入，并输出目标文本序列，本质上是一个语言模型，通常使用 Recurrent Neural Network Language Model (RNNLM)，同样也会用 LSTM、GRU 等模型。</p></li><li><p>Attention Mechanism</p><p>Encoder 输出的向量更多地表示输入序列中最后一个单词的意思，因此加入注意力机制有助于该向量更多地关注其中重要的单词。</p></li><li><p>整体思路</p><ol type="1"><li><p>将自动文摘问题构造成 seq2seq 问题，一种做法是将某段文本的第一个句子作为输入，headlines 作为输出，变成 headlines generative 问题。</p></li><li><p>选择大规模语料库作为数据集。</p></li><li><p>选择合适的 Encoder。</p></li><li><p>选择合适的 Decoder。</p></li><li><p>设计合适的 attention model。</p></li><li><p>设计 copy net。由于测试时部分词汇可能不在训练的单词表里，因此需要用 copy net 将输入的词 copy 到最终输出。</p></li></ol></li></ul><p>论文：</p><ol type="1"><li><p>(EMNLP 2015) A Neural Attention Model for Abstractive Sentence Summarization</p></li><li><p>(ICLR 2018) A Deep Reinforced Model for Abstractive Summarization</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本自动摘要（自动文摘）Text Summarization 指自动地从原始文档中提取摘要，摘要是全面准确地反映该文档中心内容的简单连贯的短文。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>DuReader</title>
    <link href="http://lijiancheng0614.github.io/2018/05/02/2018_05_02_DuReader/"/>
    <id>http://lijiancheng0614.github.io/2018/05/02/2018_05_02_DuReader/</id>
    <published>2018-05-01T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.808Z</updated>
    
    <content type="html"><![CDATA[<p>DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications</p><p>Paper: <a href="https://arxiv.org/abs/1711.05073" class="uri" target="_blank" rel="noopener">https://arxiv.org/abs/1711.05073</a></p><p>Page: <a href="http://ai.baidu.com/broad/subordinate?dataset=dureader" class="uri" target="_blank" rel="noopener">http://ai.baidu.com/broad/subordinate?dataset=dureader</a></p><p>Code: <a href="https://github.com/baidu/DuReader/" class="uri" target="_blank" rel="noopener">https://github.com/baidu/DuReader/</a></p><a id="more"></a><p>DuReader，一个新的大型开放中文机器阅读理解数据集。</p><p>DuReader 与以前的 MRC 数据集相比有三个优势：</p><ol type="1"><li><p>数据来源：问题和文档均基于百度搜索和百度知道; 答案是手动生成的。</p></li><li><p>问题类型：它为更多的问题类型提供了丰富的注释，特别是是非类和观点类问题。</p></li><li><p>规模：包含 200K 个问题，420K 个答案和 1M 个文档; 是目前最大的中文 MRC 数据集。</p></li></ol><p>This paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC.</p><p>DuReader has three advantages over previous MRC datasets:</p><ol type="1"><li><p>data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated.</p></li><li><p>question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community.</p></li><li><p>scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far.</p></li></ol><h1 id="中文">中文</h1><h2 id="简介">简介</h2><table><thead><tr class="header"><th>Dataset</th><th>Lang</th><th>#Que.</th><th>#Docs</th><th>Source of Que.</th><th>Source of Docs</th><th>Answer Type</th></tr></thead><tbody><tr class="odd"><td>CNN/DM</td><td>EN</td><td>1.4M</td><td>300K</td><td>Synthetic cloze</td><td>News</td><td>Fill in entity</td></tr><tr class="even"><td>HLF-RC</td><td>ZH</td><td>100K</td><td>28K</td><td>Synthetic cloze</td><td>Fairy/News</td><td>Fill in word</td></tr><tr class="odd"><td>CBT</td><td>EN</td><td>688K</td><td>108</td><td>Synthetic cloze</td><td>Children’s books</td><td>Multi. choices</td></tr><tr class="even"><td>RACE</td><td>EN</td><td>870K</td><td>50K</td><td>English exam</td><td>English exam</td><td>Multi. choices</td></tr><tr class="odd"><td>MCTest</td><td>EN</td><td>2K</td><td>500</td><td>Crowd-sourced</td><td>Fictional stories</td><td>Multi. choices</td></tr><tr class="even"><td>NewsQA</td><td>EN</td><td>100K</td><td>10K</td><td>Crowd-sourced</td><td>CNN</td><td>Span of words</td></tr><tr class="odd"><td>SQuAD</td><td>EN</td><td>100K</td><td>536</td><td>Crowd-sourced</td><td>Wiki.</td><td>Span of words</td></tr><tr class="even"><td>SearchQA</td><td>EN</td><td>140K</td><td>6.9M</td><td>QA site</td><td>Web doc.</td><td>Span of words</td></tr><tr class="odd"><td>TrivaQA</td><td>EN</td><td>40K</td><td>660K</td><td>Trivia websites</td><td>Wiki./Web doc.</td><td>Span/substring of words</td></tr><tr class="even"><td>NarrativeQA</td><td>EN</td><td>46K</td><td>1.5K</td><td>Crowd-sourced</td><td>Book&amp;movie</td><td>Manual summary</td></tr><tr class="odd"><td>MS-MARCO</td><td>EN</td><td>100K</td><td>200K</td><td>User logs</td><td>Web doc.</td><td>Manual summary</td></tr><tr class="even"><td>DuReader</td><td>ZH</td><td>200k</td><td>1M</td><td>User logs</td><td>Web doc./CQA</td><td>Manual summary</td></tr></tbody></table><p>表 1: 机器阅读理解数据集对比</p><h2 id="初步研究">初步研究</h2><table><colgroup><col style="width: 17%"><col style="width: 41%"><col style="width: 41%"></colgroup><thead><tr class="header"><th>Examples</th><th>Fact</th><th>Opinion</th></tr></thead><tbody><tr class="odd"><td>Entity</td><td>iphone哪天发布</td><td>2017最好看的十部电影</td></tr><tr class="even"><td>-</td><td>On which day will iphone be released</td><td>Top 10 movies of 2017</td></tr><tr class="odd"><td>Description</td><td>消防车为什么是红的</td><td>丰田卡罗拉怎么样</td></tr><tr class="even"><td>-</td><td>Why are firetrucks red</td><td>How is Toyota Carola</td></tr><tr class="odd"><td>YesNo</td><td>39.5度算高烧吗</td><td>学围棋能开发智力吗</td></tr><tr class="even"><td>-</td><td>Is 39.5 degree a high fever</td><td>Does learning to play go improve intelligence</td></tr></tbody></table><p>表 2: 中文六类问题的例子</p><h2 id="从初步研究到-dureader">从初步研究到 DuReader</h2><h3 id="数据收集与标注">数据收集与标注</h3><h4 id="数据收集">数据收集</h4><p>DuReader 的样本可用四元组表示： <span class="math inline">\(\{q, t, D, A\}\)</span>，其中 <span class="math inline">\(q\)</span> 是问题，<span class="math inline">\(t\)</span> 是问题类型，<span class="math inline">\(D\)</span> 是相关文档集合，<span class="math inline">\(A\)</span> 是由人类标注产生的答案集合。</p><h4 id="问题类型标注">问题类型标注</h4><h4 id="答案标注">答案标注</h4><p>众包</p><h4 id="质量控制">质量控制</h4><h4 id="训练集开发集和测试集">训练集、开发集和测试集</h4><table><thead><tr class="header"><th>数量</th><th>训练集</th><th>开发集</th><th>测试集</th></tr></thead><tbody><tr class="odd"><td>问题</td><td>181K</td><td>10K</td><td>10K</td></tr><tr class="even"><td>文档</td><td>855K</td><td>45K</td><td>46K</td></tr><tr class="odd"><td>答案</td><td>376K</td><td>20K</td><td>21K</td></tr></tbody></table><h3 id="dureader-是相对地很有挑战">DuReader 是（相对地）很有挑战</h3><p>挑战：</p><ol type="1"><li><p>答案数量</p><p><img src="figure1.png"></p><p>图 1. 答案数量分布</p></li><li><p>编辑距离</p><p>人类生成的答案和源文档之间的差异很大。</p></li><li><p>文档长度</p><p>问题平均 4.8 词，答案平均 69.6 词，文档平均 396 词。</p></li></ol><h2 id="实验">实验</h2><h3 id="基线系统">基线系统</h3><ol type="1"><li><p>从每个文件中选择一个最相关的段落</p></li><li><p>在选定的段落中应用最先进的 MRC 模型</p></li></ol><h3 id="段落选择">段落选择</h3><p>在训练阶段，我们从文档中选择与人类生成答案重叠最大的段落作为最相关段落。</p><p>在测试阶段，由于我们没有人类生成答案，我们选择与问题重叠最大的段落作为最相关段落。</p><h3 id="答案选择">答案选择</h3><ul><li><p>Match-LSTM</p><p>要在段落中找到答案，它会按顺序遍历段落，并动态地将注意力加权问题表示与段落的每个标记进行匹配。</p><p>最后，使用答案指针层来查找段落中的答案范围。</p></li><li><p>BiDAF</p><p>它使用上下文对问题的关注和问题对上下文的关注，以突出问题和上下文中的重要部分。</p><p>之后，使用注意流层来融合所有有用的信息，以获得每个位置的向量表示。</p></li></ul><h3 id="结果和分析">结果和分析</h3><p>评价方法：BLEU-4, Rouge-L</p><table><thead><tr class="header"><th>Systems</th><th>Baidu Search</th><th>-</th><th>Baidu Zhidao</th><th>-</th><th>All</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Selected Paragraph</td><td>15.8</td><td>22.6</td><td>16.5</td><td>38.3</td><td>16.4</td><td>30.2</td></tr><tr class="odd"><td>Match-LSTM</td><td>23.1</td><td>31.2</td><td>42.5</td><td>48.0</td><td>31.9</td><td>39.2</td></tr><tr class="even"><td>BiDAF</td><td>23.1</td><td>31.1</td><td>42.2</td><td>47.5</td><td>31.8</td><td>39.0</td></tr><tr class="odd"><td>Human</td><td>55.1</td><td>54.4</td><td>57.1</td><td>60.7</td><td>56.1</td><td>57.4</td></tr></tbody></table><p>表 6：典型的 MRC 系统在 DuReader 上的效果</p><table><thead><tr class="header"><th>Question type</th><th>Description</th><th>-</th><th>Entity</th><th>-</th><th>YesNo</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Match-LSTM</td><td>32.8</td><td>40.0</td><td>29.5</td><td>38.5</td><td>5.9</td><td>7.2</td></tr><tr class="odd"><td>BiDAF</td><td>32.6</td><td>39.7</td><td>29.8</td><td>38.4</td><td>5.5</td><td>7.5</td></tr><tr class="even"><td>Human</td><td>58.1</td><td>58.0</td><td>44.6</td><td>52.0</td><td>56.2</td><td>57.4</td></tr></tbody></table><p>表 8：不同问题类型的效果</p><h3 id="opinion-aware-验证">Opinion-aware 验证</h3><table><thead><tr class="header"><th>Question type</th><th>Fact</th><th>-</th><th>Opinion</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Opinion-unaware</td><td>6.3</td><td>8.3</td><td>5.0</td><td>7.1</td></tr><tr class="odd"><td>Opinion-aware</td><td>12.0</td><td>13.9</td><td>8.0</td><td>8.9</td></tr></tbody></table><p>表 9：opinion-aware 模型在 YesNo 问题上的效果</p><h3 id="讨论">讨论</h3><h2 id="结论">结论</h2><p>提出了 DuReader 数据集，提供了几个 baseline。</p><h1 id="english">English</h1><h2 id="introduction">Introduction</h2><table><thead><tr class="header"><th>Dataset</th><th>Lang</th><th>#Que.</th><th>#Docs</th><th>Source of Que.</th><th>Source of Docs</th><th>Answer Type</th></tr></thead><tbody><tr class="odd"><td>CNN/DM</td><td>EN</td><td>1.4M</td><td>300K</td><td>Synthetic cloze</td><td>News</td><td>Fill in entity</td></tr><tr class="even"><td>HLF-RC</td><td>ZH</td><td>100K</td><td>28K</td><td>Synthetic cloze</td><td>Fairy/News</td><td>Fill in word</td></tr><tr class="odd"><td>CBT</td><td>EN</td><td>688K</td><td>108</td><td>Synthetic cloze</td><td>Children’s books</td><td>Multi. choices</td></tr><tr class="even"><td>RACE</td><td>EN</td><td>870K</td><td>50K</td><td>English exam</td><td>English exam</td><td>Multi. choices</td></tr><tr class="odd"><td>MCTest</td><td>EN</td><td>2K</td><td>500</td><td>Crowd-sourced</td><td>Fictional stories</td><td>Multi. choices</td></tr><tr class="even"><td>NewsQA</td><td>EN</td><td>100K</td><td>10K</td><td>Crowd-sourced</td><td>CNN</td><td>Span of words</td></tr><tr class="odd"><td>SQuAD</td><td>EN</td><td>100K</td><td>536</td><td>Crowd-sourced</td><td>Wiki.</td><td>Span of words</td></tr><tr class="even"><td>SearchQA</td><td>EN</td><td>140K</td><td>6.9M</td><td>QA site</td><td>Web doc.</td><td>Span of words</td></tr><tr class="odd"><td>TrivaQA</td><td>EN</td><td>40K</td><td>660K</td><td>Trivia websites</td><td>Wiki./Web doc.</td><td>Span/substring of words</td></tr><tr class="even"><td>NarrativeQA</td><td>EN</td><td>46K</td><td>1.5K</td><td>Crowd-sourced</td><td>Book&amp;movie</td><td>Manual summary</td></tr><tr class="odd"><td>MS-MARCO</td><td>EN</td><td>100K</td><td>200K</td><td>User logs</td><td>Web doc.</td><td>Manual summary</td></tr><tr class="even"><td>DuReader</td><td>ZH</td><td>200k</td><td>1M</td><td>User logs</td><td>Web doc./CQA</td><td>Manual summary</td></tr></tbody></table><p>Table 1: DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search &amp; Baidu Zhidao; answers are manually generated, (2) question types, and (3) scale: 200k questions, 420k answers and 1M documents (largest Chinese MRC dataset so far). The next three tables address advantage (2).</p><h2 id="pilot-study">Pilot Study</h2><table><colgroup><col style="width: 17%"><col style="width: 41%"><col style="width: 41%"></colgroup><thead><tr class="header"><th>Examples</th><th>Fact</th><th>Opinion</th></tr></thead><tbody><tr class="odd"><td>Entity</td><td>iphone哪天发布</td><td>2017最好看的十部电影</td></tr><tr class="even"><td>-</td><td>On which day will iphone be released</td><td>Top 10 movies of 2017</td></tr><tr class="odd"><td>Description</td><td>消防车为什么是红的</td><td>丰田卡罗拉怎么样</td></tr><tr class="even"><td>-</td><td>Why are firetrucks red</td><td>How is Toyota Carola</td></tr><tr class="odd"><td>YesNo</td><td>39.5度算高烧吗</td><td>学围棋能开发智力吗</td></tr><tr class="even"><td>-</td><td>Is 39.5 degree a high fever</td><td>Does learning to play go improve intelligence</td></tr></tbody></table><p>Table 2: Examples of the six types of questions in Chinese (with glosses in English). Previous datasets have focused on fact-entity and fact-description, though all six types are common in search logs.</p><h2 id="scaling-up-from-the-pilot-to-dureader">Scaling up from the Pilot to DuReader</h2><h3 id="data-collection-and-annotation">Data Collection and Annotation</h3><h4 id="data-collection">Data Collection</h4><p>The DuReader is a sequence of 4-tuples: <span class="math inline">\(\{q, t, D, A\}\)</span>, where <span class="math inline">\(q\)</span> is a question, <span class="math inline">\(t\)</span> is a question type, <span class="math inline">\(D\)</span> is a set of relevant documents, and <span class="math inline">\(A\)</span> is an answer set produced by human annotators.</p><h4 id="question-type-annotation">Question Type Annotation</h4><h4 id="answer-annotation">Answer Annotation</h4><p>Crowd-sourcing</p><h4 id="quality-control">Quality Control</h4><h4 id="training-development-and-test-sets">Training, Development and Test Sets</h4><table><thead><tr class="header"><th>number</th><th>training</th><th>development</th><th>test</th></tr></thead><tbody><tr class="odd"><td>questions</td><td>181K</td><td>10K</td><td>10K</td></tr><tr class="even"><td>documents</td><td>855K</td><td>45K</td><td>46K</td></tr><tr class="odd"><td>answers</td><td>376K</td><td>20K</td><td>21K</td></tr></tbody></table><p>The training, development and test sets consist of 181K, 10K and 10K questions, 855K, 45K and 46K documents, 376K, 20K and 21K answers, respectively.</p><h3 id="dureader-is-relatively-challenging">DuReader is (Relatively) Challenging</h3><p>challenges:</p><ol type="1"><li><p>The number of answers.</p><p><img src="figure1.png"></p><p>Figure 1: A few questions have one (and only one) answer, especially for Zhidao.</p></li><li><p>The edit distance.</p><p>the difference between the human generated answers and the source documents is large.</p></li><li><p>The document length.</p><p>In DuReader, questions tend to be short (4.8 words on average) compared to answers (69.6 words), and answers tend to be short compared to documents (396 words on average).</p></li></ol><h2 id="experiments">Experiments</h2><h3 id="baseline-systems">Baseline Systems</h3><p>our designed systems have two steps:</p><ol type="1"><li><p>select one most related paragraph from each document</p></li><li><p>apply the state-of-the-art MRC models on the selected paragraphs</p></li></ol><h3 id="paragraph-selection">Paragraph Selection</h3><p>In training stage, we select one paragraph from a document as the most relevant one, if the paragraph has the largest overlap with human generated answer.</p><p>In testing stage, since we have no human generated answer, we select the most relevant paragraph that has the largest overlap with the corresponding question.</p><h3 id="answer-span-selection">Answer Span Selection</h3><ul><li><p>Match-LSTM</p><p>To find an answer in a paragraph, it goes through the paragraph sequentially and dynamically aggregates the matching of an attention-weighted question representation to each token of the paragraph.</p><p>Finally, an answer pointer layer is used to find an answer span in the paragraph.</p></li><li><p>BiDAF</p><p>It uses both context-to-question attention and question-to-context attention in order to highlight the important parts in both question and context.</p><p>After that, the so-called attention flow layer is used to fuse all useful information in order to get a vector representation for each position.</p></li></ul><h3 id="results-and-analysis">Results and Analysis</h3><p>We evaluate the reading comprehension task via character-level BLEU-4 and Rouge-L.</p><table><thead><tr class="header"><th>Systems</th><th>Baidu Search</th><th>-</th><th>Baidu Zhidao</th><th>-</th><th>All</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Selected Paragraph</td><td>15.8</td><td>22.6</td><td>16.5</td><td>38.3</td><td>16.4</td><td>30.2</td></tr><tr class="odd"><td>Match-LSTM</td><td>23.1</td><td>31.2</td><td>42.5</td><td>48.0</td><td>31.9</td><td>39.2</td></tr><tr class="even"><td>BiDAF</td><td>23.1</td><td>31.1</td><td>42.2</td><td>47.5</td><td>31.8</td><td>39.0</td></tr><tr class="odd"><td>Human</td><td>55.1</td><td>54.4</td><td>57.1</td><td>60.7</td><td>56.1</td><td>57.4</td></tr></tbody></table><p>Table 6: Performance of typical MRC systems on the DuReader.</p><table><thead><tr class="header"><th>Question type</th><th>Description</th><th>-</th><th>Entity</th><th>-</th><th>YesNo</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Match-LSTM</td><td>32.8</td><td>40.0</td><td>29.5</td><td>38.5</td><td>5.9</td><td>7.2</td></tr><tr class="odd"><td>BiDAF</td><td>32.6</td><td>39.7</td><td>29.8</td><td>38.4</td><td>5.5</td><td>7.5</td></tr><tr class="even"><td>Human</td><td>58.1</td><td>58.0</td><td>44.6</td><td>52.0</td><td>56.2</td><td>57.4</td></tr></tbody></table><p>Table 8: Performance on various question types.</p><h3 id="opinion-aware-evaluation">Opinion-aware Evaluation</h3><table><thead><tr class="header"><th>Question type</th><th>Fact</th><th>-</th><th>Opinion</th><th>-</th></tr></thead><tbody><tr class="odd"><td>-</td><td>BLEU-4%</td><td>Rouge-L%</td><td>BLEU-4%</td><td>Rouge-L%</td></tr><tr class="even"><td>Opinion-unaware</td><td>6.3</td><td>8.3</td><td>5.0</td><td>7.1</td></tr><tr class="odd"><td>Opinion-aware</td><td>12.0</td><td>13.9</td><td>8.0</td><td>8.9</td></tr></tbody></table><p>Table 9: Performance of opinion-aware model on YesNo questions.</p><h3 id="discussion">Discussion</h3><h2 id="conclusion">Conclusion</h2><p>This paper announced the release of DuReader, a new dataset for researchers interested in machine reading comprehension (MRC).</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications&lt;/p&gt;
&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1711.05073&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1711.05073&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Page: &lt;a href=&quot;http://ai.baidu.com/broad/subordinate?dataset=dureader&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://ai.baidu.com/broad/subordinate?dataset=dureader&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;https://github.com/baidu/DuReader/&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/baidu/DuReader/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://lijiancheng0614.github.io/tags/NLP/"/>
    
      <category term="Paper" scheme="http://lijiancheng0614.github.io/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>XPS 15 安装 Windows 10 + Ubuntu 双系统</title>
    <link href="http://lijiancheng0614.github.io/2018/04/29/2018_04_29_XPS15_Win10_Ubuntu/"/>
    <id>http://lijiancheng0614.github.io/2018/04/29/2018_04_29_XPS15_Win10_Ubuntu/</id>
    <published>2018-04-28T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.808Z</updated>
    
    <content type="html"><![CDATA[<p>在 Dell XPS 15 上安装 Windows 10 + Ubuntu 双系统，主要把默认安装的 Windows 修改到 AHCI 模式下。</p><a id="more"></a><h2 id="安装环境">安装环境</h2><p>以下为成功安装的环境：</p><ul><li><p>Dell XPS 15</p></li><li><p>默认安装的 Windows 10</p></li><li><p>Ubuntu 16.04</p></li></ul><h2 id="安装过程">安装过程</h2><h3 id="把默认安装的-windows-修改到-ahci-模式下">把默认安装的 Windows 修改到 AHCI 模式下</h3><ol type="1"><li><p>在 Windows 下，以<strong>安全模式</strong>重启</p><blockquote><p>一种进入安全模式的方法：运行 <code>msconfig</code>，选择 <code>安全模式</code>，重启<br>其它方法：<a href="https://support.microsoft.com/zh-cn/help/12376/windows-10-start-your-pc-in-safe-mode" class="uri" target="_blank" rel="noopener">https://support.microsoft.com/zh-cn/help/12376/windows-10-start-your-pc-in-safe-mode</a></p></blockquote></li><li><p>重启，按 F2 进入启动设置</p><p>进入 System configuration &gt; SATA operation，把默认的 <code>RAID On</code> 选项改成 <code>AHCI</code></p></li><li><p>重启，等待 Windows 自动修复即可</p><blockquote><p>如在 <code>msconfig</code> 中修改了以安全模式进入，可修改回来</p></blockquote></li></ol><h3 id="安装-ubuntu">安装 Ubuntu</h3><ol type="1"><li><p>开机按 F2 进入启动设置</p><p>进入 General &gt; Boot Sequence，选择 <code>Boot List Option</code> 里的 <code>UEFI</code> 选项</p><p>进入 Secure Boot &gt; Secure Boot，选择 <code>Secure Boot Enable</code> 里的 <code>Disabled</code> 选项</p><blockquote><p>如需详细步骤见 <a href="http://www.dell.com/support/article/cn/zh/cndhs1/sln301754/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%88%B4%E5%B0%94pc%E4%B8%8A%E5%AE%89%E8%A3%85ubuntu%E5%92%8Cwindows8%E6%88%9610%E4%BD%9C%E4%B8%BA%E5%8F%8C%E5%BC%95%E5%AF%BC?lang=zh" class="uri" target="_blank" rel="noopener">http://www.dell.com/support/article/cn/zh/cndhs1/sln301754/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%88%B4%E5%B0%94pc%E4%B8%8A%E5%AE%89%E8%A3%85ubuntu%E5%92%8Cwindows8%E6%88%9610%E4%BD%9C%E4%B8%BA%E5%8F%8C%E5%BC%95%E5%AF%BC?lang=zh</a></p></blockquote></li><li><p>在 Windows 下，进入磁盘管理，压缩卷预留一些空间以安装 Ubuntu</p><blockquote><p>如需详细步骤见 <a href="https://support.microsoft.com/zh-cn/help/944248" class="uri" target="_blank" rel="noopener">https://support.microsoft.com/zh-cn/help/944248</a><br>但不需要创建新分区，得到未分配空间即可<br><img src="https://support.microsoft.com/Library/Images/2474497.jpg"></p></blockquote></li><li><p>下载 Ubuntu 安装包，制作安装 Ubuntu 的 U盘启动盘，如使用 UltroISO 刻录</p><blockquote><p>下载地址 <a href="https://www.ubuntu.com/download/desktop" class="uri" target="_blank" rel="noopener">https://www.ubuntu.com/download/desktop</a><br>如得到 <code>ubuntu-16.04-desktop-amd64.iso</code></p></blockquote></li><li><p>重启，按 F12，选择 U盘启动</p></li><li><p>安装 Ubuntu</p><blockquote><p>基本一直按下一步<br>安装详细步骤见 <a href="https://tutorials.ubuntu.com/tutorial/tutorial-install-ubuntu-desktop" class="uri" target="_blank" rel="noopener">https://tutorials.ubuntu.com/tutorial/tutorial-install-ubuntu-desktop</a></p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Dell XPS 15 上安装 Windows 10 + Ubuntu 双系统，主要把默认安装的 Windows 修改到 AHCI 模式下。&lt;/p&gt;
    
    </summary>
    
      <category term="Default" scheme="http://lijiancheng0614.github.io/categories/Default/"/>
    
    
      <category term="Ubuntu" scheme="http://lijiancheng0614.github.io/tags/Ubuntu/"/>
    
      <category term="Windows" scheme="http://lijiancheng0614.github.io/tags/Windows/"/>
    
  </entry>
  
  <entry>
    <title>修改TensorFlow-DeepLab</title>
    <link href="http://lijiancheng0614.github.io/2018/03/16/2018_03_16_TensorFlow-DeepLab/"/>
    <id>http://lijiancheng0614.github.io/2018/03/16/2018_03_16_TensorFlow-DeepLab/</id>
    <published>2018-03-15T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.808Z</updated>
    
    <content type="html"><![CDATA[<p>代码仓库：<a href="https://github.com/lijiancheng0614/tensorflow_deeplab" class="uri" target="_blank" rel="noopener">https://github.com/lijiancheng0614/tensorflow_deeplab</a></p><p>修改TensorFlow DeepLab，添加一些方便使用或新的功能。</p><a id="more"></a><h2 id="中文">中文</h2><p>使用方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/lijiancheng0614/tensorflow_deeplab deeplab</span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`/models/research/slim</span><br></pre></td></tr></table></figure><h3 id="在eval.py中添加gpu_allow_growth参数">在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>eval.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><code>eval.py</code></li></ul><h3 id="在train.py中添加gpu_allow_growth参数">在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数</h3><p>在<code>train.py</code>中添加<code>gpu_allow_growth</code>参数，默认为<code>True</code>，即不占用GPU全部内存，而是动态申请显存。</p><p>修改文件：</p><ul><li><code>train.py</code></li></ul><h3 id="在train.py中添加max_to_keep参数">在<code>train.py</code>中添加<code>max_to_keep</code>参数</h3><p>在<code>train.py</code>中添加<code>max_to_keep</code>参数，默认为<code>5</code>，即保留最后5个checkpoint。如为<code>0</code>则保留所有的checkpoint。</p><p>修改文件：</p><ul><li><code>train.py</code></li></ul><h2 id="english">English</h2><p>Usage:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/lijiancheng0614/tensorflow_deeplab deeplab</span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`/models/research/slim</span><br></pre></td></tr></table></figure><h3 id="add-gpu_allow_growth-parameter-in-eval.py">Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>eval.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>eval.py</code></p></li><li><p><code>evaluator.py</code></p></li><li><p><code>eval_util.py</code></p></li></ul><h3 id="add-gpu_allow_growth-parameter-in-train.py">Add <code>gpu_allow_growth</code> parameter in <code>train.py</code></h3><p>Add <code>gpu_allow_growth</code> parameter in <code>train.py</code>, default value is <code>True</code> which means attempting to allocate only as much GPU memory based on runtime allocations.</p><p>Modified files:</p><ul><li><p><code>train.py</code></p></li><li><p><code>trainer.py</code></p></li></ul><h3 id="add-max_to_keep-parameter-in-train.py">Add <code>max_to_keep</code> parameter in <code>train.py</code></h3><p>Add <code>max_to_keep</code> parameter in <code>train.py</code>, default value is <code>5</code> which means the 5 most recent checkpoint files are kept. If <code>0</code>, all checkpoint files are kept.</p><p>Modified files:</p><ul><li><code>train.py</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;代码仓库：&lt;a href=&quot;https://github.com/lijiancheng0614/tensorflow_deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/lijiancheng0614/tensorflow_deeplab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;修改TensorFlow DeepLab，添加一些方便使用或新的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow DeepLab进行语义分割</title>
    <link href="http://lijiancheng0614.github.io/2018/03/13/2018_03_13_TensorFlow-DeepLab/"/>
    <id>http://lijiancheng0614.github.io/2018/03/13/2018_03_13_TensorFlow-DeepLab/</id>
    <published>2018-03-12T16:00:00.000Z</published>
    <updated>2025-08-08T03:42:05.807Z</updated>
    
    <content type="html"><![CDATA[<p>参考 <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" class="uri" target="_blank" rel="noopener">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p><p>使用 TensorFlow DeepLab 进行语义分割</p><a id="more"></a><p><img src="https://raw.githubusercontent.com/tensorflow/models/master/research/deeplab/g3doc/img/vis1.png"></p><h2 id="准备">准备</h2><ol start="0" type="1"><li><p>文件结构</p><p>这里以 PASCAL VOC 2012 为例，参考官方推荐的文件结构：</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">deeplab/datasets/pascal_voc_seg</span><br><span class="line">├── exp</span><br><span class="line">│   └── train_on_train_set</span><br><span class="line">│       ├── eval</span><br><span class="line">│       │   └── events.out.tfevents....</span><br><span class="line">│       ├── export</span><br><span class="line">│       │   └── frozen_inference_graph.pb</span><br><span class="line">│       ├── train</span><br><span class="line">│       │   ├── checkpoint</span><br><span class="line">│       │   ├── events.out.tfevents....</span><br><span class="line">│       │   ├── graph.pbtxt</span><br><span class="line">│       │   ├── model.ckpt-0.data-00000-of-00001</span><br><span class="line">│       │   ├── model.ckpt-0.index</span><br><span class="line">│       │   ├── model.ckpt-0.meta</span><br><span class="line">│       │   └── ...</span><br><span class="line">│       └── vis</span><br><span class="line">│           ├── graph.pbtxt</span><br><span class="line">│           ├── raw_segmentation_results</span><br><span class="line">│           └── segmentation_results</span><br><span class="line">├── init_models</span><br><span class="line">│   └── deeplabv3_pascal_train_aug</span><br><span class="line">│       ├── frozen_inference_graph.pb</span><br><span class="line">│       ├── model.ckpt.data-00000-of-00001</span><br><span class="line">│       └── model.ckpt.index</span><br><span class="line">├── tfrecord</span><br><span class="line">│   ├── ....tfrecord</span><br><span class="line">│   └── ...</span><br><span class="line">└── VOCdevkit</span><br><span class="line">    └── VOC2012</span><br><span class="line">        ├── Annotations</span><br><span class="line">        ├── ImageSets</span><br><span class="line">        │   ├── Action</span><br><span class="line">        │   ├── Layout</span><br><span class="line">        │   ├── Main</span><br><span class="line">        │   └── Segmentation</span><br><span class="line">        ├── JPEGImages</span><br><span class="line">        ├── SegmentationClass</span><br><span class="line">        ├── SegmentationClassRaw</span><br><span class="line">        └── SegmentationObject</span><br></pre></td></tr></table></figure></p></li><li><p>安装 TensorFlow</p><p>参考 <a href="https://www.tensorflow.org/install/" class="uri" target="_blank" rel="noopener">https://www.tensorflow.org/install/</a> ，安装 TensorFlow v1.5.0 或更新的版本。</p><p>如果操作系统、GPU 型号、Python 版本号等配置跟官方一致，可直接使用官网提供的安装包安装。</p><blockquote><p>编译源码时注意 bazel 可能并不能总是获取 <code>$LD_LIBRARY_PATH</code>，如有报错，可以尝试添加参数 <code>action_env</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bazel build --config=opt --config=cuda tensorflow/tools/pip_package:build_pip_package --action_env=&quot;LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;&quot;</span><br></pre></td></tr></table></figure></p></blockquote></li><li><p>配置 TensorFlow Models</p><ul><li>下载 TensorFlow Models</li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</span><br></pre></td></tr></table></figure></p><ul><li>添加 <code>$PYTHONPATH</code></li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From tensorflow/models/research/</span></span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PYTHONPATH</span>:`<span class="built_in">pwd</span>`:`<span class="built_in">pwd</span>`/slim</span><br></pre></td></tr></table></figure></p><ul><li>测试</li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From tensorflow/models/research/</span></span><br><span class="line">python deeplab/model_test.py</span><br></pre></td></tr></table></figure></p><p>若成功，显示<code>OK</code>。</p></li><li><p>准备数据</p><p>这里以 <code>PASCAL VOC 2012</code> 为例。</p><p>参考 <a href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md" class="uri" target="_blank" rel="noopener">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md</a></p><p>运行以下代码即可：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/</span></span><br><span class="line">sh download_and_convert_voc2012.sh</span><br></pre></td></tr></table></figure></p><p>实际上，该脚本执行了以下操作：</p><ul><li>下载并解压</li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/</span></span><br><span class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar</span><br><span class="line">tar -xf VOCtrainval_11-May-2012.tar</span><br></pre></td></tr></table></figure></p><ul><li>移除 ground-truth 中的 colormap</li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/</span></span><br><span class="line">PASCAL_ROOT=<span class="string">"pascal_voc_seg/VOCdevkit/VOC2012"</span></span><br><span class="line">SEG_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/SegmentationClass"</span></span><br><span class="line">SEMANTIC_SEG_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/SegmentationClassRaw"</span></span><br><span class="line">python ./remove_gt_colormap.py \</span><br><span class="line">    --original_gt_folder=<span class="string">"<span class="variable">$&#123;SEG_FOLDER&#125;</span>"</span> \</span><br><span class="line">    --output_dir=<span class="string">"<span class="variable">$&#123;SEMANTIC_SEG_FOLDER&#125;</span>"</span></span><br></pre></td></tr></table></figure></p><ul><li>生成 TFRecord</li></ul><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/</span></span><br><span class="line">OUTPUT_DIR=<span class="string">"pascal_voc_seg/tfrecord"</span></span><br><span class="line">mkdir -p <span class="string">"<span class="variable">$&#123;OUTPUT_DIR&#125;</span>"</span></span><br><span class="line">IMAGE_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/JPEGImages"</span></span><br><span class="line">LIST_FOLDER=<span class="string">"<span class="variable">$&#123;PASCAL_ROOT&#125;</span>/ImageSets/Segmentation"</span></span><br><span class="line">python ./build_voc2012_data.py \</span><br><span class="line">    --image_folder=<span class="string">"<span class="variable">$&#123;IMAGE_FOLDER&#125;</span>"</span> \</span><br><span class="line">    --semantic_segmentation_folder=<span class="string">"<span class="variable">$&#123;SEMANTIC_SEG_FOLDER&#125;</span>"</span> \</span><br><span class="line">    --list_folder=<span class="string">"<span class="variable">$&#123;LIST_FOLDER&#125;</span>"</span> \</span><br><span class="line">    --image_format=<span class="string">"jpg"</span> \</span><br><span class="line">    --output_dir=<span class="string">"<span class="variable">$&#123;OUTPUT_DIR&#125;</span>"</span></span><br></pre></td></tr></table></figure></p></li><li><p>（可选）下载模型</p><p>官方提供了不少预训练模型（ <a href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md" class="uri" target="_blank" rel="noopener">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md</a> ），</p><p>这里以 <code>deeplabv3_pascal_train_aug_2018_01_04</code> 以例。</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/</span></span><br><span class="line">mkdir init_models</span><br><span class="line"><span class="built_in">cd</span> init_models</span><br><span class="line">wget http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz</span><br><span class="line">tar zxf ssd_mobilenet_v1_coco_11_06_2017.tar.gz</span><br></pre></td></tr></table></figure></p></li></ol><h2 id="训练">训练</h2><p>如果使用现有模型进行预测则不需要训练。</p><ol type="1"><li><p>训练</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/train.sh</code>，内容如下：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p logs/</span><br><span class="line">now=$(date +<span class="string">"%Y%m%d_%H%M%S"</span>)</span><br><span class="line">python ../../../../train.py \</span><br><span class="line">    --logtostderr \</span><br><span class="line">    --train_split=<span class="string">"train"</span> \</span><br><span class="line">    --model_variant=<span class="string">"xception_65"</span> \</span><br><span class="line">    --atrous_rates=6 \</span><br><span class="line">    --atrous_rates=12 \</span><br><span class="line">    --atrous_rates=18 \</span><br><span class="line">    --output_stride=16 \</span><br><span class="line">    --decoder_output_stride=4 \</span><br><span class="line">    --train_crop_size=513 \</span><br><span class="line">    --train_crop_size=513 \</span><br><span class="line">    --train_batch_size=4 \</span><br><span class="line">    --training_number_of_steps=10 \</span><br><span class="line">    --fine_tune_batch_norm=<span class="literal">false</span> \</span><br><span class="line">    --tf_initial_checkpoint=<span class="string">"../../init_models/deeplabv3_pascal_train_aug/model.ckpt"</span> \</span><br><span class="line">    --train_logdir=<span class="string">"train/"</span> \</span><br><span class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> 2&gt;&amp;1 | tee logs/train_<span class="variable">$now</span>.txt &amp;</span><br></pre></td></tr></table></figure></p><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh train.sh</code> 即可训练。</p></li><li><p>验证</p><p>可一边训练一边验证，注意使用其它的GPU或合理分配显存。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/eval.sh</code>，内容如下：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">python ../../../../eval.py \</span><br><span class="line">    --logtostderr \</span><br><span class="line">    --eval_split=<span class="string">"val"</span> \</span><br><span class="line">    --model_variant=<span class="string">"xception_65"</span> \</span><br><span class="line">    --atrous_rates=6 \</span><br><span class="line">    --atrous_rates=12 \</span><br><span class="line">    --atrous_rates=18 \</span><br><span class="line">    --output_stride=16 \</span><br><span class="line">    --decoder_output_stride=4 \</span><br><span class="line">    --eval_crop_size=513 \</span><br><span class="line">    --eval_crop_size=513 \</span><br><span class="line">    --checkpoint_dir=<span class="string">"train/"</span> \</span><br><span class="line">    --eval_logdir=<span class="string">"eval/"</span> \</span><br><span class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> &amp;</span><br><span class="line">    <span class="comment"># --max_number_of_evaluations=1 &amp;</span></span><br></pre></td></tr></table></figure></p><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>CUDA_VISIBLE_DEVICES="1" sh eval.sh</code> 即可验证（这里指定了第二个 GPU）。</p></li><li><p>可视化 log</p><p>可一边训练一边可视化训练的 log，访问 <code>http://localhost:6006/</code> 即可看到 loss 等的变化。</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></span><br><span class="line">tensorboard --logdir train/</span><br></pre></td></tr></table></figure></p><p>可视化验证的 log，可看到 <code>miou_1.0</code> 的变化，这里指定了另一个端口。</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></span><br><span class="line">tensorboard --logdir <span class="built_in">eval</span>/ --port 6007</span><br></pre></td></tr></table></figure></p><p>或同时可视化训练与验证的log：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set</span></span><br><span class="line">tensorboard --logdir .</span><br></pre></td></tr></table></figure></p></li><li><p>可视化分割结果</p><p>可一边训练一边可视化分割结果。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/vis.sh</code>，内容如下：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">python ../../../../vis.py \</span><br><span class="line">    --logtostderr \</span><br><span class="line">    --vis_split=<span class="string">"val"</span> \</span><br><span class="line">    --model_variant=<span class="string">"xception_65"</span> \</span><br><span class="line">    --atrous_rates=6 \</span><br><span class="line">    --atrous_rates=12 \</span><br><span class="line">    --atrous_rates=18 \</span><br><span class="line">    --output_stride=16 \</span><br><span class="line">    --decoder_output_stride=4 \</span><br><span class="line">    --vis_crop_size=513 \</span><br><span class="line">    --vis_crop_size=513 \</span><br><span class="line">    --checkpoint_dir=<span class="string">"train/"</span> \</span><br><span class="line">    --vis_logdir=<span class="string">"vis/"</span> \</span><br><span class="line">    --dataset_dir=<span class="string">"../../tfrecord/"</span> &amp;</span><br><span class="line">    <span class="comment"># --max_number_of_evaluations=1 &amp;</span></span><br></pre></td></tr></table></figure></p><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh vis.sh</code> 即可生成分割结果，<code>vis/segmentation_results/</code> 里有彩色化的分割结果，<code>vis/raw_segmentation_results/</code> 里有原始的分割结果。</p></li></ol><h2 id="测试">测试</h2><ol type="1"><li><p>导出模型</p><p>训练完成后得到一些 checkpoint 文件在 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/train/</code> 中，如：</p><ul><li>graph.pbtxt</li><li>model.ckpt-1000.data-00000-of-00001</li><li>model.ckpt-1000.info</li><li>model.ckpt-1000.meta</li></ul><p>其中 meta 文件保存了 graph 和 metadata，ckpt 文件保存了网络的 weights。</p><p>而进行预测时只需模型和权重，不需要 metadata，故可使用官方提供的脚本生成推导图。</p><p>新建 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/export_model.sh</code>，内容如下：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">python ../../../../export_model.py \</span><br><span class="line">    --logtostderr \</span><br><span class="line">    --checkpoint_path=<span class="string">"train/model.ckpt-<span class="variable">$1</span>"</span> \</span><br><span class="line">    --export_path=<span class="string">"export/frozen_inference_graph-<span class="variable">$1</span>.pb"</span> \</span><br><span class="line">    --model_variant=<span class="string">"xception_65"</span> \</span><br><span class="line">    --atrous_rates=6 \</span><br><span class="line">    --atrous_rates=12 \</span><br><span class="line">    --atrous_rates=18 \</span><br><span class="line">    --output_stride=16 \</span><br><span class="line">    --decoder_output_stride=4 \</span><br><span class="line">    --num_classes=21 \</span><br><span class="line">    --crop_size=513 \</span><br><span class="line">    --crop_size=513 \</span><br><span class="line">    --inference_scales=1.0</span><br></pre></td></tr></table></figure></p><p>进入 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</code>，</p><p>运行 <code>sh export_model.sh 1000</code> 即可导出模型 <code>export/frozen_inference_graph-1000.pb</code>。</p></li><li><p>测试图片</p><ul><li><p>运行 <code>deeplab_demo.ipynb</code> 并修改其中的各种路径即可。</p></li><li><p>或自写 inference 脚本，如 <code>deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/infer.py</code></p><p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'../../../../utils/'</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> get_dataset_colormap</span><br><span class="line"></span><br><span class="line">LABEL_NAMES = np.asarray([</span><br><span class="line">    <span class="string">'background'</span>, <span class="string">'aeroplane'</span>, <span class="string">'bicycle'</span>, <span class="string">'bird'</span>, <span class="string">'boat'</span>, <span class="string">'bottle'</span>, <span class="string">'bus'</span>,</span><br><span class="line">    <span class="string">'car'</span>, <span class="string">'cat'</span>, <span class="string">'chair'</span>, <span class="string">'cow'</span>, <span class="string">'diningtable'</span>, <span class="string">'dog'</span>, <span class="string">'horse'</span>, <span class="string">'motorbike'</span>,</span><br><span class="line">    <span class="string">'person'</span>, <span class="string">'pottedplant'</span>, <span class="string">'sheep'</span>, <span class="string">'sofa'</span>, <span class="string">'train'</span>, <span class="string">'tv'</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), <span class="number">1</span>)</span><br><span class="line">FULL_COLOR_MAP = get_dataset_colormap.label_to_color_image(FULL_LABEL_MAP)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepLabModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Class to load deeplab model and run inference."""</span></span><br><span class="line"></span><br><span class="line">    INPUT_TENSOR_NAME = <span class="string">'ImageTensor:0'</span></span><br><span class="line">    OUTPUT_TENSOR_NAME = <span class="string">'SemanticPredictions:0'</span></span><br><span class="line">    INPUT_SIZE = <span class="number">513</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_path)</span>:</span></span><br><span class="line">        <span class="string">"""Creates and loads pretrained deeplab model."""</span></span><br><span class="line">        self.graph = tf.Graph()</span><br><span class="line">        <span class="keyword">with</span> open(model_path) <span class="keyword">as</span> fd:</span><br><span class="line">            graph_def = tf.GraphDef.FromString(fd.read())</span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default():</span><br><span class="line">            tf.import_graph_def(graph_def, name=<span class="string">''</span>)</span><br><span class="line">        self.sess = tf.Session(graph=self.graph)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, image)</span>:</span></span><br><span class="line">        <span class="string">"""Runs inference on a single image.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            image: A PIL.Image object, raw input image.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            resized_image: RGB image resized from original input image.</span></span><br><span class="line"><span class="string">            seg_map: Segmentation map of `resized_image`.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        width, height = image.size</span><br><span class="line">        resize_ratio = <span class="number">1.0</span> * self.INPUT_SIZE / max(width, height)</span><br><span class="line">        target_size = (int(resize_ratio * width), int(resize_ratio * height))</span><br><span class="line">        resized_image = image.convert(<span class="string">'RGB'</span>).resize(target_size,</span><br><span class="line">                                                    Image.ANTIALIAS)</span><br><span class="line">        batch_seg_map = self.sess.run(</span><br><span class="line">            self.OUTPUT_TENSOR_NAME,</span><br><span class="line">            feed_dict=&#123;</span><br><span class="line">                self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]</span><br><span class="line">            &#125;)</span><br><span class="line">        seg_map = batch_seg_map[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> resized_image, seg_map</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_segmentation</span><span class="params">(image, seg_map)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">221</span>)</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.title(<span class="string">'input image'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">222</span>)</span><br><span class="line">    seg_image = get_dataset_colormap.label_to_color_image(</span><br><span class="line">        seg_map, get_dataset_colormap.get_pascal_name()).astype(np.uint8)</span><br><span class="line">    plt.imshow(seg_image)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.title(<span class="string">'segmentation map'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">223</span>)</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.imshow(seg_image, alpha=<span class="number">0.7</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.title(<span class="string">'segmentation overlay'</span>)</span><br><span class="line"></span><br><span class="line">    unique_labels = np.unique(seg_map)</span><br><span class="line">    ax = plt.subplot(<span class="number">224</span>)</span><br><span class="line">    plt.imshow(</span><br><span class="line">        FULL_COLOR_MAP[unique_labels].astype(np.uint8),</span><br><span class="line">        interpolation=<span class="string">'nearest'</span>)</span><br><span class="line">    ax.yaxis.tick_right()</span><br><span class="line">    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])</span><br><span class="line">    plt.xticks([], [])</span><br><span class="line">    ax.tick_params(width=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</span><br><span class="line">        print(<span class="string">'Usage: python &#123;&#125; image_path model_path'</span>.format(sys.argv[<span class="number">0</span>]))</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line">    image_path = sys.argv[<span class="number">1</span>]</span><br><span class="line">    model_path = sys.argv[<span class="number">2</span>]</span><br><span class="line">    model = DeepLabModel(model_path)</span><br><span class="line">    orignal_im = Image.open(image_path)</span><br><span class="line">    resized_im, seg_map = model.run(orignal_im)</span><br><span class="line">    vis_segmentation(resized_im, seg_map)</span><br></pre></td></tr></table></figure></p><p>运行以下命令即可：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/</span></span><br><span class="line">python infer.py \</span><br><span class="line">    ../../../../g3doc/img/image1.jpg \</span><br><span class="line">    <span class="built_in">export</span>/frozen_inference_graph.pb</span><br></pre></td></tr></table></figure></p><p>运行结果：</p><p><img src="plot_image1.png"></p></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/deeplab&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tensorflow/models/tree/master/research/deeplab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用 TensorFlow DeepLab 进行语义分割&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="http://lijiancheng0614.github.io/categories/Research/"/>
    
    
      <category term="Computer Vision" scheme="http://lijiancheng0614.github.io/tags/Computer-Vision/"/>
    
      <category term="Deep Learning" scheme="http://lijiancheng0614.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://lijiancheng0614.github.io/tags/TensorFlow/"/>
    
  </entry>
  
</feed>
